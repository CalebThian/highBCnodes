{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbf6a13",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb22d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206514a0",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing  \n",
    "Data Structure:\n",
    "1. **gList** <Dict>: containing total 31 graphs, which 30 from Synthetic and 1 from youtube,using filename as key  \n",
    "2. element of gList <Dict>: 'graph':nx.Graph();'score': <Dict> with 'node' and 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b0a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt has 5000 nodes, 19982 edges\n",
      "1.txt has 5000 nodes, 19981 edges\n",
      "10.txt has 5000 nodes, 19980 edges\n",
      "11.txt has 5000 nodes, 19983 edges\n",
      "12.txt has 5000 nodes, 19983 edges\n",
      "13.txt has 5000 nodes, 19984 edges\n",
      "14.txt has 5000 nodes, 19982 edges\n",
      "15.txt has 5000 nodes, 19984 edges\n",
      "16.txt has 5000 nodes, 19982 edges\n",
      "17.txt has 5000 nodes, 19981 edges\n",
      "18.txt has 5000 nodes, 19984 edges\n",
      "19.txt has 5000 nodes, 19981 edges\n",
      "2.txt has 5000 nodes, 19980 edges\n",
      "20.txt has 5000 nodes, 19983 edges\n",
      "21.txt has 5000 nodes, 19982 edges\n",
      "22.txt has 5000 nodes, 19982 edges\n",
      "23.txt has 5000 nodes, 19981 edges\n",
      "24.txt has 5000 nodes, 19984 edges\n",
      "25.txt has 5000 nodes, 19982 edges\n",
      "26.txt has 5000 nodes, 19984 edges\n",
      "27.txt has 5000 nodes, 19983 edges\n",
      "28.txt has 5000 nodes, 19982 edges\n",
      "29.txt has 5000 nodes, 19983 edges\n",
      "3.txt has 5000 nodes, 19982 edges\n",
      "4.txt has 5000 nodes, 19984 edges\n",
      "5.txt has 5000 nodes, 19981 edges\n",
      "6.txt has 5000 nodes, 19984 edges\n",
      "7.txt has 5000 nodes, 19983 edges\n",
      "8.txt has 5000 nodes, 19983 edges\n",
      "9.txt has 5000 nodes, 19984 edges\n",
      "com-youtube.txt has 1134890 nodes, 2987624 edges\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "dpath = \".\\\\data\\\\\"\n",
    "gList = dict()\n",
    "filenames = []\n",
    "\n",
    "for root, dirs, files in os.walk(dpath):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if 'score' not in file:\n",
    "            filenames.append(file)\n",
    "            # Process nodes and edges\n",
    "            gList[file] = dict()\n",
    "            gList[file]['graph']=nx.Graph()\n",
    "            with open(file_path,'r') as f:\n",
    "                content = f.readlines()\n",
    "                edges = []\n",
    "                for line in content:\n",
    "                    if 'com' not in file:\n",
    "                        nodes = line[:-1].split('\\t')\n",
    "                    else:\n",
    "                        #continue # after finish all code run code with com\n",
    "                        nodes = line[:-1].split(\" \")\n",
    "                    # Create edge tuple and append\n",
    "                    edges.append((int(nodes[0]),int(nodes[1])))\n",
    "                gList[file]['graph'].add_edges_from(edges)\n",
    "                print(\"{} has {} nodes, {} edges\".format(file,gList[file]['graph'].number_of_nodes(),gList[file]['graph'].number_of_edges()))\n",
    "            \n",
    "            # Process scores\n",
    "            scorefile = file.replace(\".txt\",\"_score.txt\")\n",
    "            gList[file]['score'] = dict()\n",
    "            score_file_path = os.path.join(root,scorefile) \n",
    "            with open(score_file_path,'r') as f:\n",
    "                content = f.readlines()\n",
    "                for line in content:\n",
    "                    if 'com' not in file:\n",
    "                        node_score = line[:-1].split('\\t')\n",
    "                        gList[file]['score'][int(node_score[0])] = float(node_score[1])\n",
    "                    else:\n",
    "                        #continue # after finish all code run code with com\n",
    "                        node_score = line[:-1].split()\n",
    "                        gList[file]['score'][int(node_score[0][:-1])] = float(node_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd0956",
   "metadata": {},
   "source": [
    "# 3. DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e286c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gList['0.txt']['graph']\n",
    "y = torch.tensor(list(gList['0.txt']['score'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72b7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare nodes initial feature X [dv,1,1]\n",
    "def gen_nodes_feature(G):\n",
    "    deg = np.array(list(dict(sorted(dict(G.degree()).items())).values()))\n",
    "    X = np.ones((3,len(deg)))\n",
    "    X[0,:]=deg\n",
    "    norms = np.linalg.norm(X,axis = 1,keepdims=True)\n",
    "    X = torch.FloatTensor(X.T)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d99f2d",
   "metadata": {},
   "source": [
    "## 3a. DrBC encoder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edd220ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the model\\ninput_size = 3\\nhidden_size = 32\\nnum_layers = 5\\nencoder = DrBCEncoder(input_size, hidden_size, num_layers,g)\\nX = gen_nodes_feature(g)\\nout = encoder(X)\\nprint(out.shape)\\nprint(out)\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DrBCEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,G):\n",
    "        super(DrBCEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.gru_cells = nn.GRUCell(hidden_size, hidden_size)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.G = G\n",
    "        self.deg = dict(self.G.degree())\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        output = [x]\n",
    "        for i in range(self.num_layers-1):\n",
    "            hn = self.calHn(x)\n",
    "            x = self.gru_cells(x,hn)\n",
    "            x = self.norm2(x)\n",
    "            output.append(x)\n",
    "        output, _ = torch.max(torch.stack(output), dim=0)\n",
    "        return output\n",
    "\n",
    "    def calHn(self,x):\n",
    "        hn = torch.zeros(x.shape).to(self.device)\n",
    "        for node in self.G.nodes():\n",
    "            degv = self.deg[node]\n",
    "            for neigh in list(self.G.adj[node]):\n",
    "                denominator = 1/(math.sqrt(degv+1)*math.sqrt(self.deg[neigh]+1))\n",
    "                hn[node,:] += (denominator*x[neigh])\n",
    "        return hn\n",
    "    \n",
    "'''\n",
    "# Define the model\n",
    "input_size = 3\n",
    "hidden_size = 32\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,g)\n",
    "X = gen_nodes_feature(g)\n",
    "out = encoder(X)\n",
    "print(out.shape)\n",
    "print(out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a37a5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrBCEncoder(\n",
      "  (layer1): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (gru_cells): GRUCell(128, 128)\n",
      "  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 3\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ebdc3",
   "metadata": {},
   "source": [
    "## 3b. Decoder: 2-layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a345a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrBCDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DrBCDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the layers of the decoder\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.norm2 = nn.BatchNorm1d(output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the layers of the decoder\n",
    "        x = self.layer1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e17db1",
   "metadata": {},
   "source": [
    "## 3c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d8e81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(minimum,maximum,qty):\n",
    "    pairs = []\n",
    "    for i in range(qty):\n",
    "        a = random.randint(minimum,maximum)\n",
    "        b = random.randint(minimum,maximum)\n",
    "        pairs.append((a,b))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "300596cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bc_pairs(pairs, outputs, y):\n",
    "    pred = []\n",
    "    gt = []\n",
    "    g = nn.Sigmoid()\n",
    "    for i, pair in enumerate(pairs):\n",
    "        pred.append(g(outputs[pair[0]] - outputs[pair[1]]))\n",
    "        gt.append(g(y[pair[0]] - y[pair[1]]))\n",
    "    return torch.stack(pred), torch.stack(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ec0e095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nG = gList['0.txt']['graph']\\ny = torch.tensor([list(gList['0.txt']['score'].values())])\\ny = torch.transpose(y,0,1)\\n# Define the models\\ninput_size = 3\\nhidden_size = 128\\noutput_size = 1\\nnum_layers = 5\\nencoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\\ndecoder = DrBCDecoder(hidden_size,hidden_size,output_size)\\n\\nn = G.number_of_nodes()\\nnum_episodes = 20\\nlr = 0.001\\nsample_qty = 5*n\\n\\n# Define the loss and optimizer\\ncriterion = nn.BCELoss(reduction = 'sum')\\noptimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\\n\\n# Get the inputs\\ninputs = gen_nodes_feature(G)\\n\\n# Train the model\\nfor episode in range(num_episodes):\\n    # model\\n    outputs = encoder(inputs)\\n    outputs = decoder(outputs)\\n    \\n    pairs = sampling(0,n-1,sample_qty)\\n    pred,gt = bc_pairs(pairs,outputs,y)\\n    loss = criterion(pred,gt)\\n\\n    if ~loss.requires_grad:\\n        loss.requires_grad_()\\n        \\n    # Zero the parameter gradients\\n    optimizer.zero_grad()\\n    loss.backward()\\n    optimizer.step()\\n\\n    # Print statistics\\n    print('[%d] loss: %.4f' %(episode + 1, loss.item()))\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "G = gList['0.txt']['graph']\n",
    "y = torch.tensor([list(gList['0.txt']['score'].values())])\n",
    "y = torch.transpose(y,0,1)\n",
    "# Define the models\n",
    "input_size = 3\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "decoder = DrBCDecoder(hidden_size,hidden_size,output_size)\n",
    "\n",
    "n = G.number_of_nodes()\n",
    "num_episodes = 20\n",
    "lr = 0.001\n",
    "sample_qty = 5*n\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.BCELoss(reduction = 'sum')\n",
    "optimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Get the inputs\n",
    "inputs = gen_nodes_feature(G)\n",
    "\n",
    "# Train the model\n",
    "for episode in range(num_episodes):\n",
    "    # model\n",
    "    outputs = encoder(inputs)\n",
    "    outputs = decoder(outputs)\n",
    "    \n",
    "    pairs = sampling(0,n-1,sample_qty)\n",
    "    pred,gt = bc_pairs(pairs,outputs,y)\n",
    "    loss = criterion(pred,gt)\n",
    "\n",
    "    if ~loss.requires_grad:\n",
    "        loss.requires_grad_()\n",
    "        \n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print statistics\n",
    "    print('[%d] loss: %.4f' %(episode + 1, loss.item()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342b506",
   "metadata": {},
   "source": [
    "# 4. Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682e9af",
   "metadata": {},
   "source": [
    "## 4a. Top-N% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73a618b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(n,pred,gt):\n",
    "    k = math.ceil(pred.size()[0]*n/100)\n",
    "    _,pred_top = torch.topk(pred.view(-1),k=k)\n",
    "    _,gt_top = torch.topk(gt.view(-1),k=k)\n",
    "    intersect = torch.unique(torch.cat((pred_top,gt_top),0))\n",
    "    acc = (2*k-len(intersect))/k\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4c84e",
   "metadata": {},
   "source": [
    "## 4b. Kendall tau distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2f0dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall(pred,gt):\n",
    "    pred_ind = torch.argsort(pred)\n",
    "    gt_ind = torch.argsort(gt)\n",
    "    con = 0 # number of concordant pairs\n",
    "    dcor = 0 # number of discordant pairs\n",
    "    n = len(pred_ind)\n",
    "    for i in range(n):\n",
    "        if pred_ind[i] == gt_ind[i]:\n",
    "            con += 1\n",
    "        else:\n",
    "            dcor += 1\n",
    "    ken = 2*(con-dcor)/(n*(n-1))\n",
    "    return ken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e3bda",
   "metadata": {},
   "source": [
    "## 4c. Wall-clock running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c2e6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# The implementation is combine with the final function \n",
    "# that will be defined afterward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b437c0",
   "metadata": {},
   "source": [
    "# 5. Putting all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ffab299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33cff5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_result(filename):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    k = [1,5,10]\n",
    "    \n",
    "    print(\"Working on {}\".format(filename))\n",
    "    # Prepare the data\n",
    "    G = gList[filename]['graph']\n",
    "    y = torch.tensor([list(gList[filename]['score'].values())])\n",
    "    y = torch.transpose(y,0,1)\n",
    "\n",
    "    # Define the models\n",
    "    input_size = 3\n",
    "    hidden_size = 128\n",
    "    output_size = 1\n",
    "    num_layers = 5\n",
    "    encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "    decoder = DrBCDecoder(hidden_size,hidden_size,output_size)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    model = EncoderDecoder(encoder,decoder)\n",
    "    model.to(device)\n",
    "    \n",
    "    n = G.number_of_nodes()\n",
    "    num_episodes = 20\n",
    "    lr = 0.001\n",
    "    sample_qty = 5*n\n",
    "\n",
    "    # Define the loss and optimizer\n",
    "    criterion = nn.BCELoss(reduction = 'sum')\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction = 'sum') # Because only pred will go through sigmoid, but ground truth won't\n",
    "    #optimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get the inputs\n",
    "    inputs = gen_nodes_feature(G)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Start training on {}\".format(device))\n",
    "    pairs = sampling(0,n-1,sample_qty)\n",
    "    for episode in range(num_episodes):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # model\n",
    "        outputs = model(inputs)\n",
    "        pred,gt = bc_pairs(pairs,outputs,y)\n",
    "        loss = criterion(pred,gt)\n",
    "        if ~loss.requires_grad:\n",
    "            loss.requires_grad_()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if best_loss>loss:\n",
    "            best_loss = loss\n",
    "            best_out = outputs\n",
    "            #best_model_weights = model.state_dict()\n",
    "            \n",
    "        # Print statistics\n",
    "        #if episode%5 == 4:\n",
    "        print('[%d] loss: %.4f' %(episode + 1, loss.item()))\n",
    "            \n",
    "    #torch.save(best_model_weights, 'best_model.pth')\n",
    "    top1 = topN(1,best_out,y)\n",
    "    top5 = topN(5,best_out,y)\n",
    "    top10 = topN(10,best_out,y)\n",
    "    ken = kendall(best_out,y)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "    return top1,top5,top10,ken,elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ba66c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21299.3242\n",
      "[2] loss: 20293.3203\n",
      "[3] loss: 19834.7461\n",
      "[4] loss: 20113.3145\n",
      "[5] loss: 19897.2500\n",
      "[6] loss: 19670.0391\n",
      "[7] loss: 19622.2090\n",
      "[8] loss: 19558.9668\n",
      "[9] loss: 19447.5742\n",
      "[10] loss: 19196.9609\n",
      "[11] loss: 19075.3496\n",
      "[12] loss: 19002.4883\n",
      "[13] loss: 18924.7070\n",
      "[14] loss: 18872.3555\n",
      "[15] loss: 18871.8789\n",
      "[16] loss: 18807.5664\n",
      "[17] loss: 18825.5742\n",
      "[18] loss: 18782.5781\n",
      "[19] loss: 18779.6562\n",
      "[20] loss: 18753.7617\n",
      "Elapsed time: 688.62 seconds\n",
      "Working on 1.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21423.6211\n",
      "[2] loss: 20592.6738\n",
      "[3] loss: 20577.1133\n",
      "[4] loss: 20718.1211\n",
      "[5] loss: 20638.0352\n",
      "[6] loss: 20539.4785\n",
      "[7] loss: 20480.2754\n",
      "[8] loss: 20426.3945\n",
      "[9] loss: 20355.5352\n",
      "[10] loss: 20243.2383\n",
      "[11] loss: 20065.2578\n",
      "[12] loss: 19572.7617\n",
      "[13] loss: 19522.2227\n",
      "[14] loss: 19483.7812\n",
      "[15] loss: 19448.4453\n",
      "[16] loss: 19326.6797\n",
      "[17] loss: 19170.7754\n",
      "[18] loss: 18969.4336\n",
      "[19] loss: 18927.1445\n",
      "[20] loss: 18865.3711\n",
      "Elapsed time: 681.05 seconds\n",
      "Working on 10.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22213.4570\n",
      "[2] loss: 20632.7461\n",
      "[3] loss: 19887.5391\n",
      "[4] loss: 19744.6992\n",
      "[5] loss: 19487.9180\n",
      "[6] loss: 19427.2383\n",
      "[7] loss: 19411.8457\n",
      "[8] loss: 19313.7852\n",
      "[9] loss: 19059.3750\n",
      "[10] loss: 18954.1875\n",
      "[11] loss: 18917.1953\n",
      "[12] loss: 18887.4277\n",
      "[13] loss: 18858.3203\n",
      "[14] loss: 18818.2656\n",
      "[15] loss: 18776.5508\n",
      "[16] loss: 18742.9102\n",
      "[17] loss: 18720.5469\n",
      "[18] loss: 18658.2188\n",
      "[19] loss: 18054.3828\n",
      "[20] loss: 18002.0684\n",
      "Elapsed time: 679.06 seconds\n",
      "Working on 11.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22459.2344\n",
      "[2] loss: 20915.1074\n",
      "[3] loss: 19161.7715\n",
      "[4] loss: 18874.5430\n",
      "[5] loss: 18858.5898\n",
      "[6] loss: 18802.7734\n",
      "[7] loss: 18773.6758\n",
      "[8] loss: 18741.0137\n",
      "[9] loss: 18701.9102\n",
      "[10] loss: 18653.7852\n",
      "[11] loss: 18478.7734\n",
      "[12] loss: 18450.5918\n",
      "[13] loss: 18419.4727\n",
      "[14] loss: 18391.6738\n",
      "[15] loss: 18364.7480\n",
      "[16] loss: 18340.2051\n",
      "[17] loss: 18194.5000\n",
      "[18] loss: 18180.1035\n",
      "[19] loss: 18160.7402\n",
      "[20] loss: 18142.9648\n",
      "Elapsed time: 690.69 seconds\n",
      "Working on 12.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22343.6055\n",
      "[2] loss: 20290.9062\n",
      "[3] loss: 19964.4023\n",
      "[4] loss: 19828.3184\n",
      "[5] loss: 19693.5625\n",
      "[6] loss: 19713.7715\n",
      "[7] loss: 19626.3203\n",
      "[8] loss: 19536.6133\n",
      "[9] loss: 19490.1758\n",
      "[10] loss: 19409.7617\n",
      "[11] loss: 19250.2227\n",
      "[12] loss: 19177.9141\n",
      "[13] loss: 19112.4551\n",
      "[14] loss: 19060.8359\n",
      "[15] loss: 19014.8496\n",
      "[16] loss: 18739.7500\n",
      "[17] loss: 18680.6602\n",
      "[18] loss: 18493.2734\n",
      "[19] loss: 18116.3203\n",
      "[20] loss: 18034.2383\n",
      "Elapsed time: 678.83 seconds\n",
      "Working on 13.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21791.2852\n",
      "[2] loss: 20119.6992\n",
      "[3] loss: 19508.3535\n",
      "[4] loss: 19638.9238\n",
      "[5] loss: 19285.9102\n",
      "[6] loss: 18821.6562\n",
      "[7] loss: 18692.5508\n",
      "[8] loss: 18575.8867\n",
      "[9] loss: 18476.5625\n",
      "[10] loss: 18398.5977\n",
      "[11] loss: 18339.7910\n",
      "[12] loss: 18286.6133\n",
      "[13] loss: 18259.4648\n",
      "[14] loss: 18242.8633\n",
      "[15] loss: 18234.7500\n",
      "[16] loss: 18228.5527\n",
      "[17] loss: 18209.1289\n",
      "[18] loss: 18194.7676\n",
      "[19] loss: 18186.2168\n",
      "[20] loss: 18176.0742\n",
      "Elapsed time: 680.23 seconds\n",
      "Working on 14.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21122.7578\n",
      "[2] loss: 20770.9805\n",
      "[3] loss: 19878.4453\n",
      "[4] loss: 19703.0234\n",
      "[5] loss: 19650.7344\n",
      "[6] loss: 19517.8047\n",
      "[7] loss: 19291.6094\n",
      "[8] loss: 19163.6816\n",
      "[9] loss: 19102.4023\n",
      "[10] loss: 19046.6250\n",
      "[11] loss: 18999.8789\n",
      "[12] loss: 18957.4785\n",
      "[13] loss: 18896.7773\n",
      "[14] loss: 18850.1367\n",
      "[15] loss: 18806.7969\n",
      "[16] loss: 18556.3184\n",
      "[17] loss: 18544.2832\n",
      "[18] loss: 18496.8398\n",
      "[19] loss: 18462.7676\n",
      "[20] loss: 18431.1211\n",
      "Elapsed time: 685.75 seconds\n",
      "Working on 15.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22500.3184\n",
      "[2] loss: 21325.8555\n",
      "[3] loss: 20584.8926\n",
      "[4] loss: 20435.8379\n",
      "[5] loss: 20101.7344\n",
      "[6] loss: 19912.4238\n",
      "[7] loss: 19762.5781\n",
      "[8] loss: 19648.1309\n",
      "[9] loss: 19561.6094\n",
      "[10] loss: 19487.4766\n",
      "[11] loss: 19407.3516\n",
      "[12] loss: 19332.3711\n",
      "[13] loss: 19263.2988\n",
      "[14] loss: 19195.0312\n",
      "[15] loss: 18947.4688\n",
      "[16] loss: 18609.5918\n",
      "[17] loss: 18506.9219\n",
      "[18] loss: 18425.7930\n",
      "[19] loss: 18390.5977\n",
      "[20] loss: 18362.8203\n",
      "Elapsed time: 684.74 seconds\n",
      "Working on 16.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21546.2090\n",
      "[2] loss: 21277.2793\n",
      "[3] loss: 20751.4180\n",
      "[4] loss: 20561.1777\n",
      "[5] loss: 20527.4023\n",
      "[6] loss: 20475.6074\n",
      "[7] loss: 20424.8867\n",
      "[8] loss: 20388.3086\n",
      "[9] loss: 20350.6094\n",
      "[10] loss: 20315.8594\n",
      "[11] loss: 20287.2617\n",
      "[12] loss: 20257.7109\n",
      "[13] loss: 20232.0273\n",
      "[14] loss: 20207.8105\n",
      "[15] loss: 20185.1816\n",
      "[16] loss: 20192.6680\n",
      "[17] loss: 19597.4102\n",
      "[18] loss: 19560.0352\n",
      "[19] loss: 19136.0918\n",
      "[20] loss: 19094.9688\n",
      "Elapsed time: 688.36 seconds\n",
      "Working on 17.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20669.6055\n",
      "[2] loss: 19836.2266\n",
      "[3] loss: 19529.4414\n",
      "[4] loss: 19369.8320\n",
      "[5] loss: 19296.1211\n",
      "[6] loss: 19233.1426\n",
      "[7] loss: 18989.3672\n",
      "[8] loss: 18961.2734\n",
      "[9] loss: 18935.1211\n",
      "[10] loss: 18903.0469\n",
      "[11] loss: 18869.3242\n",
      "[12] loss: 18837.3047\n",
      "[13] loss: 18812.8945\n",
      "[14] loss: 18419.7812\n",
      "[15] loss: 18391.9336\n",
      "[16] loss: 18362.9570\n",
      "[17] loss: 18336.1484\n",
      "[18] loss: 18299.1641\n",
      "[19] loss: 18255.2266\n",
      "[20] loss: 18222.4062\n",
      "Elapsed time: 687.35 seconds\n",
      "Working on 18.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21000.0977\n",
      "[2] loss: 20434.4336\n",
      "[3] loss: 20178.1406\n",
      "[4] loss: 20241.1680\n",
      "[5] loss: 20101.5195\n",
      "[6] loss: 19966.7461\n",
      "[7] loss: 19926.1758\n",
      "[8] loss: 19697.2344\n",
      "[9] loss: 19572.8047\n",
      "[10] loss: 19483.2070\n",
      "[11] loss: 19388.5117\n",
      "[12] loss: 19318.0586\n",
      "[13] loss: 19297.7227\n",
      "[14] loss: 19250.8027\n",
      "[15] loss: 19199.5312\n",
      "[16] loss: 18293.3008\n",
      "[17] loss: 18522.9844\n",
      "[18] loss: 18664.1016\n",
      "[19] loss: 19170.8086\n",
      "[20] loss: 19167.9375\n",
      "Elapsed time: 691.73 seconds\n",
      "Working on 19.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22102.8477\n",
      "[2] loss: 20576.7383\n",
      "[3] loss: 19873.6816\n",
      "[4] loss: 19729.1543\n",
      "[5] loss: 19560.4629\n",
      "[6] loss: 19457.8750\n",
      "[7] loss: 19413.8672\n",
      "[8] loss: 19350.3145\n",
      "[9] loss: 18908.4102\n",
      "[10] loss: 18871.8027\n",
      "[11] loss: 18809.8047\n",
      "[12] loss: 18630.4492\n",
      "[13] loss: 18492.1914\n",
      "[14] loss: 18471.0352\n",
      "[15] loss: 18544.9023\n",
      "[16] loss: 18505.0430\n",
      "[17] loss: 18469.1094\n",
      "[18] loss: 18302.4395\n",
      "[19] loss: 18175.4609\n",
      "[20] loss: 18175.7676\n",
      "Elapsed time: 689.54 seconds\n",
      "Working on 2.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22154.6875\n",
      "[2] loss: 20700.0195\n",
      "[3] loss: 20589.3105\n",
      "[4] loss: 20278.6250\n",
      "[5] loss: 20135.7500\n",
      "[6] loss: 20054.3945\n",
      "[7] loss: 19967.4023\n",
      "[8] loss: 19803.7520\n",
      "[9] loss: 19643.7559\n",
      "[10] loss: 19588.2031\n",
      "[11] loss: 19541.9512\n",
      "[12] loss: 19501.2539\n",
      "[13] loss: 19458.2344\n",
      "[14] loss: 19434.1152\n",
      "[15] loss: 19386.1953\n",
      "[16] loss: 19368.3281\n",
      "[17] loss: 18900.6152\n",
      "[18] loss: 18869.2188\n",
      "[19] loss: 18838.3672\n",
      "[20] loss: 18636.5195\n",
      "Elapsed time: 682.26 seconds\n",
      "Working on 20.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21842.3750\n",
      "[2] loss: 20631.3516\n",
      "[3] loss: 20156.5957\n",
      "[4] loss: 19920.4805\n",
      "[5] loss: 19802.3730\n",
      "[6] loss: 19728.7012\n",
      "[7] loss: 19670.5059\n",
      "[8] loss: 19618.5723\n",
      "[9] loss: 19572.5410\n",
      "[10] loss: 19497.7305\n",
      "[11] loss: 19391.4961\n",
      "[12] loss: 19365.2773\n",
      "[13] loss: 19354.3281\n",
      "[14] loss: 19318.0703\n",
      "[15] loss: 19284.0898\n",
      "[16] loss: 19081.8672\n",
      "[17] loss: 19059.6113\n",
      "[18] loss: 18819.6348\n",
      "[19] loss: 18811.9707\n",
      "[20] loss: 18518.7461\n",
      "Elapsed time: 692.70 seconds\n",
      "Working on 21.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21419.7227\n",
      "[2] loss: 20466.8262\n",
      "[3] loss: 19690.7676\n",
      "[4] loss: 19523.3125\n",
      "[5] loss: 19442.9609\n",
      "[6] loss: 19384.0977\n",
      "[7] loss: 19300.0312\n",
      "[8] loss: 19236.3828\n",
      "[9] loss: 19073.1055\n",
      "[10] loss: 18996.2441\n",
      "[11] loss: 18962.0742\n",
      "[12] loss: 18910.4707\n",
      "[13] loss: 18854.9805\n",
      "[14] loss: 18817.6016\n",
      "[15] loss: 18794.1953\n",
      "[16] loss: 18758.6602\n",
      "[17] loss: 18548.1113\n",
      "[18] loss: 18492.4395\n",
      "[19] loss: 18455.1660\n",
      "[20] loss: 18425.1992\n",
      "Elapsed time: 681.30 seconds\n",
      "Working on 22.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21633.8203\n",
      "[2] loss: 20389.9082\n",
      "[3] loss: 19766.7012\n",
      "[4] loss: 19585.2012\n",
      "[5] loss: 19434.3184\n",
      "[6] loss: 19315.2617\n",
      "[7] loss: 19220.0000\n",
      "[8] loss: 19156.2988\n",
      "[9] loss: 19075.8438\n",
      "[10] loss: 19008.7188\n",
      "[11] loss: 18955.7812\n",
      "[12] loss: 18910.2656\n",
      "[13] loss: 18846.8457\n",
      "[14] loss: 18809.9355\n",
      "[15] loss: 18777.6934\n",
      "[16] loss: 18770.2402\n",
      "[17] loss: 18744.2500\n",
      "[18] loss: 18729.0898\n",
      "[19] loss: 18723.3281\n",
      "[20] loss: 18707.3398\n",
      "Elapsed time: 681.79 seconds\n",
      "Working on 23.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21282.6133\n",
      "[2] loss: 20543.0332\n",
      "[3] loss: 20241.5352\n",
      "[4] loss: 20049.4375\n",
      "[5] loss: 19938.5117\n",
      "[6] loss: 19810.9844\n",
      "[7] loss: 19699.3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] loss: 19609.6055\n",
      "[9] loss: 19525.5879\n",
      "[10] loss: 19224.6172\n",
      "[11] loss: 19155.1758\n",
      "[12] loss: 19098.3730\n",
      "[13] loss: 19051.4570\n",
      "[14] loss: 18996.3438\n",
      "[15] loss: 18942.3242\n",
      "[16] loss: 18913.7852\n",
      "[17] loss: 18905.1328\n",
      "[18] loss: 18848.3184\n",
      "[19] loss: 18674.0000\n",
      "[20] loss: 18855.9531\n",
      "Elapsed time: 678.45 seconds\n",
      "Working on 24.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22355.7891\n",
      "[2] loss: 20697.1953\n",
      "[3] loss: 20376.0547\n",
      "[4] loss: 20254.7402\n",
      "[5] loss: 20122.3398\n",
      "[6] loss: 20013.2891\n",
      "[7] loss: 19859.1797\n",
      "[8] loss: 19737.7188\n",
      "[9] loss: 19626.5586\n",
      "[10] loss: 19518.3242\n",
      "[11] loss: 19439.8691\n",
      "[12] loss: 19196.6211\n",
      "[13] loss: 19142.7285\n",
      "[14] loss: 18794.9219\n",
      "[15] loss: 18753.7695\n",
      "[16] loss: 18490.3477\n",
      "[17] loss: 18716.0508\n",
      "[18] loss: 18713.5781\n",
      "[19] loss: 18662.0117\n",
      "[20] loss: 18415.9648\n",
      "Elapsed time: 679.51 seconds\n",
      "Working on 25.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22402.2148\n",
      "[2] loss: 21026.3711\n",
      "[3] loss: 20166.7812\n",
      "[4] loss: 19919.0273\n",
      "[5] loss: 19628.1367\n",
      "[6] loss: 19479.2676\n",
      "[7] loss: 19385.6680\n",
      "[8] loss: 19285.4844\n",
      "[9] loss: 19115.6934\n",
      "[10] loss: 19026.5742\n",
      "[11] loss: 18758.9883\n",
      "[12] loss: 18694.3555\n",
      "[13] loss: 18612.0469\n",
      "[14] loss: 18536.0273\n",
      "[15] loss: 18453.5742\n",
      "[16] loss: 18394.6562\n",
      "[17] loss: 18336.7793\n",
      "[18] loss: 18306.4648\n",
      "[19] loss: 18274.9766\n",
      "[20] loss: 18269.3086\n",
      "Elapsed time: 649.22 seconds\n",
      "Working on 26.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21943.0078\n",
      "[2] loss: 20726.7188\n",
      "[3] loss: 20391.9785\n",
      "[4] loss: 20379.5859\n",
      "[5] loss: 20330.4395\n",
      "[6] loss: 20231.1328\n",
      "[7] loss: 20153.8145\n",
      "[8] loss: 20107.8516\n",
      "[9] loss: 20097.8750\n",
      "[10] loss: 20060.6816\n",
      "[11] loss: 20057.6973\n",
      "[12] loss: 20011.7695\n",
      "[13] loss: 19887.2773\n",
      "[14] loss: 19676.2949\n",
      "[15] loss: 19622.7051\n",
      "[16] loss: 19573.9453\n",
      "[17] loss: 19477.5312\n",
      "[18] loss: 19429.3379\n",
      "[19] loss: 19377.2227\n",
      "[20] loss: 19345.7891\n",
      "Elapsed time: 653.88 seconds\n",
      "Working on 27.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21985.3027\n",
      "[2] loss: 20854.7109\n",
      "[3] loss: 20285.4316\n",
      "[4] loss: 20050.5742\n",
      "[5] loss: 19987.0000\n",
      "[6] loss: 19944.0234\n",
      "[7] loss: 19884.1992\n",
      "[8] loss: 19829.3125\n",
      "[9] loss: 19767.7734\n",
      "[10] loss: 19709.9043\n",
      "[11] loss: 19668.7422\n",
      "[12] loss: 19624.0430\n",
      "[13] loss: 19568.5898\n",
      "[14] loss: 19505.0039\n",
      "[15] loss: 19438.0762\n",
      "[16] loss: 19388.1758\n",
      "[17] loss: 19328.3145\n",
      "[18] loss: 19106.1406\n",
      "[19] loss: 19061.4961\n",
      "[20] loss: 18860.9570\n",
      "Elapsed time: 645.70 seconds\n",
      "Working on 28.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20591.7305\n",
      "[2] loss: 20218.5137\n",
      "[3] loss: 19880.5938\n",
      "[4] loss: 19652.8145\n",
      "[5] loss: 19637.5840\n",
      "[6] loss: 19569.6270\n",
      "[7] loss: 19501.1680\n",
      "[8] loss: 19462.4492\n",
      "[9] loss: 19440.2031\n",
      "[10] loss: 19409.5039\n",
      "[11] loss: 19368.2305\n",
      "[12] loss: 19333.3672\n",
      "[13] loss: 19326.2266\n",
      "[14] loss: 19302.6387\n",
      "[15] loss: 19262.9766\n",
      "[16] loss: 19227.5391\n",
      "[17] loss: 19191.3633\n",
      "[18] loss: 19158.0859\n",
      "[19] loss: 18515.8262\n",
      "[20] loss: 18470.1797\n",
      "Elapsed time: 644.13 seconds\n",
      "Working on 29.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21724.5898\n",
      "[2] loss: 20960.1348\n",
      "[3] loss: 20805.2793\n",
      "[4] loss: 20607.3438\n",
      "[5] loss: 20498.0781\n",
      "[6] loss: 20406.3477\n",
      "[7] loss: 20325.2285\n",
      "[8] loss: 20303.7246\n",
      "[9] loss: 20278.8555\n",
      "[10] loss: 20264.3711\n",
      "[11] loss: 20247.4375\n",
      "[12] loss: 20151.9727\n",
      "[13] loss: 20136.9590\n",
      "[14] loss: 20002.3965\n",
      "[15] loss: 19879.5820\n",
      "[16] loss: 19838.5996\n",
      "[17] loss: 19790.4551\n",
      "[18] loss: 19738.1289\n",
      "[19] loss: 19688.4336\n",
      "[20] loss: 19654.2070\n",
      "Elapsed time: 646.97 seconds\n",
      "Working on 3.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20862.7773\n",
      "[2] loss: 19942.7676\n",
      "[3] loss: 19698.1348\n",
      "[4] loss: 19511.5293\n",
      "[5] loss: 19391.2598\n",
      "[6] loss: 19263.3867\n",
      "[7] loss: 19119.9395\n",
      "[8] loss: 19002.1133\n",
      "[9] loss: 18582.1172\n",
      "[10] loss: 18487.2109\n",
      "[11] loss: 18449.9395\n",
      "[12] loss: 18375.6367\n",
      "[13] loss: 18344.2871\n",
      "[14] loss: 18323.0938\n",
      "[15] loss: 18308.3047\n",
      "[16] loss: 18289.4199\n",
      "[17] loss: 18285.7461\n",
      "[18] loss: 18279.1230\n",
      "[19] loss: 18269.7344\n",
      "[20] loss: 18267.4316\n",
      "Elapsed time: 648.63 seconds\n",
      "Working on 4.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21585.6504\n",
      "[2] loss: 19773.6191\n",
      "[3] loss: 19371.0547\n",
      "[4] loss: 19207.7500\n",
      "[5] loss: 19028.4414\n",
      "[6] loss: 18895.5176\n",
      "[7] loss: 18790.1016\n",
      "[8] loss: 18706.9590\n",
      "[9] loss: 18329.1035\n",
      "[10] loss: 18262.4023\n",
      "[11] loss: 18227.4121\n",
      "[12] loss: 18181.1641\n",
      "[13] loss: 18123.5703\n",
      "[14] loss: 18089.7715\n",
      "[15] loss: 18076.7012\n",
      "[16] loss: 18053.6875\n",
      "[17] loss: 18043.2383\n",
      "[18] loss: 18039.3125\n",
      "[19] loss: 17842.6680\n",
      "[20] loss: 17686.1660\n",
      "Elapsed time: 652.09 seconds\n",
      "Working on 5.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21378.4785\n",
      "[2] loss: 20301.5410\n",
      "[3] loss: 19845.9609\n",
      "[4] loss: 19646.5195\n",
      "[5] loss: 19533.6113\n",
      "[6] loss: 19432.7871\n",
      "[7] loss: 19352.6758\n",
      "[8] loss: 19282.5508\n",
      "[9] loss: 19194.9082\n",
      "[10] loss: 19133.8340\n",
      "[11] loss: 19095.3359\n",
      "[12] loss: 19051.4062\n",
      "[13] loss: 19000.1777\n",
      "[14] loss: 18952.0859\n",
      "[15] loss: 18668.5254\n",
      "[16] loss: 18655.4180\n",
      "[17] loss: 18607.3320\n",
      "[18] loss: 18585.3867\n",
      "[19] loss: 18307.6758\n",
      "[20] loss: 18311.5977\n",
      "Elapsed time: 646.15 seconds\n",
      "Working on 6.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21479.5781\n",
      "[2] loss: 20711.6523\n",
      "[3] loss: 20643.0703\n",
      "[4] loss: 20504.2070\n",
      "[5] loss: 20365.3809\n",
      "[6] loss: 20225.1211\n",
      "[7] loss: 19751.1523\n",
      "[8] loss: 19697.9648\n",
      "[9] loss: 19604.0957\n",
      "[10] loss: 19541.0312\n",
      "[11] loss: 19213.2500\n",
      "[12] loss: 19066.3965\n",
      "[13] loss: 19013.9766\n",
      "[14] loss: 18708.3809\n",
      "[15] loss: 18722.5273\n",
      "[16] loss: 18828.3086\n",
      "[17] loss: 18703.8906\n",
      "[18] loss: 18588.7305\n",
      "[19] loss: 18299.4844\n",
      "[20] loss: 18269.9902\n",
      "Elapsed time: 657.59 seconds\n",
      "Working on 7.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21178.1523\n",
      "[2] loss: 20467.1777\n",
      "[3] loss: 20231.0977\n",
      "[4] loss: 20127.6172\n",
      "[5] loss: 20038.7344\n",
      "[6] loss: 19940.1836\n",
      "[7] loss: 19856.9219\n",
      "[8] loss: 19705.2539\n",
      "[9] loss: 19452.2109\n",
      "[10] loss: 19416.8164\n",
      "[11] loss: 19380.2500\n",
      "[12] loss: 19342.8926\n",
      "[13] loss: 19320.6406\n",
      "[14] loss: 19294.3398\n",
      "[15] loss: 19277.0176\n",
      "[16] loss: 19255.8594\n",
      "[17] loss: 19235.5430\n",
      "[18] loss: 19216.0508\n",
      "[19] loss: 19193.2773\n",
      "[20] loss: 19102.3242\n",
      "Elapsed time: 656.07 seconds\n",
      "Working on 8.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22349.1445\n",
      "[2] loss: 21527.0957\n",
      "[3] loss: 20833.3047\n",
      "[4] loss: 20342.7559\n",
      "[5] loss: 20107.3906\n",
      "[6] loss: 20154.6348\n",
      "[7] loss: 20204.3867\n",
      "[8] loss: 20149.0039\n",
      "[9] loss: 19855.8711\n",
      "[10] loss: 19663.1797\n",
      "[11] loss: 19420.4727\n",
      "[12] loss: 19341.7695\n",
      "[13] loss: 19261.3652\n",
      "[14] loss: 19170.6348\n",
      "[15] loss: 19092.1641\n",
      "[16] loss: 18820.7285\n",
      "[17] loss: 18738.5820\n",
      "[18] loss: 18685.1172\n",
      "[19] loss: 18606.1328\n",
      "[20] loss: 18566.3535\n",
      "Elapsed time: 658.16 seconds\n",
      "Working on 9.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22009.8027\n",
      "[2] loss: 20267.5801\n",
      "[3] loss: 19974.7422\n",
      "[4] loss: 19839.5977\n",
      "[5] loss: 19736.3477\n",
      "[6] loss: 19651.5078\n",
      "[7] loss: 19565.7305\n",
      "[8] loss: 19526.1641\n",
      "[9] loss: 19466.2949\n",
      "[10] loss: 19240.6855\n",
      "[11] loss: 19183.1328\n",
      "[12] loss: 19104.6094\n",
      "[13] loss: 18949.5469\n",
      "[14] loss: 18930.5879\n",
      "[15] loss: 18768.7188\n",
      "[16] loss: 18695.0898\n",
      "[17] loss: 18640.1211\n",
      "[18] loss: 18285.8848\n",
      "[19] loss: 17978.9297\n",
      "[20] loss: 17972.2305\n",
      "Elapsed time: 652.54 seconds\n"
     ]
    }
   ],
   "source": [
    "top1_list = []\n",
    "top5_list = []\n",
    "top10_list = []\n",
    "ken_list = []\n",
    "elapsed_time_list = []\n",
    "for filename in filenames[:-1]:\n",
    "    top1,top5,top10,ken,elapsed_time = train_and_result(filename)\n",
    "    top1_list.append(top1)\n",
    "    top5_list.append(top5)\n",
    "    top10_list.append(top10)\n",
    "    ken_list.append(ken)\n",
    "    elapsed_time_list.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e9a0bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1% accuracy: 0.28±0.18\n",
      "Top-5% accuracy: 0.19±0.09\n",
      "Top-10% accuracy: 0.22±0.07\n",
      "Kendall tau distance: 0.00±0.00\n",
      "Running time: 671.10±17.12\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean for each evaluation metrics:\n",
    "print(\"Top-1% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top1_list)),np.std(np.array(top1_list))))\n",
    "print(\"Top-5% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top5_list)),np.std(np.array(top5_list))))\n",
    "print(\"Top-10% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top10_list)),np.std(np.array(top10_list))))\n",
    "print(\"Kendall tau distance: {:.2f}±{:.2f}\".format(np.mean(np.array(ken_list)),np.std(np.array(ken_list))))\n",
    "print(\"Running time: {:.2f}±{:.2f}\".format(np.mean(np.array(elapsed_time_list)),np.std(np.array(elapsed_time_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10484bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21960.4258\n",
      "[2] loss: 20787.8262\n",
      "[3] loss: 20470.4121\n",
      "[4] loss: 20211.0039\n",
      "[5] loss: 20164.1016\n",
      "[6] loss: 19974.1680\n",
      "[7] loss: 19869.4805\n",
      "[8] loss: 19799.8301\n",
      "[9] loss: 19574.0039\n",
      "[10] loss: 19471.1328\n",
      "[11] loss: 19426.7031\n",
      "[12] loss: 19398.8164\n",
      "[13] loss: 19358.6719\n",
      "[14] loss: 19310.4258\n",
      "[15] loss: 19291.0801\n",
      "[16] loss: 19279.0352\n",
      "[17] loss: 19237.9141\n",
      "[18] loss: 19217.1562\n",
      "[19] loss: 19190.5547\n",
      "[20] loss: 18821.2188\n",
      "Elapsed time: 646.14 seconds\n",
      "Working on 1.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22176.9102\n",
      "[2] loss: 20837.9512\n",
      "[3] loss: 20642.5039\n",
      "[4] loss: 20292.6836\n",
      "[5] loss: 20147.5586\n",
      "[6] loss: 20106.0684\n",
      "[7] loss: 20116.7344\n",
      "[8] loss: 19858.1270\n",
      "[9] loss: 19707.4023\n",
      "[10] loss: 19656.1719\n",
      "[11] loss: 19575.7227\n",
      "[12] loss: 19507.5742\n",
      "[13] loss: 19440.1484\n",
      "[14] loss: 19404.9727\n",
      "[15] loss: 19344.6309\n",
      "[16] loss: 19214.3633\n",
      "[17] loss: 19026.6797\n",
      "[18] loss: 18792.1582\n",
      "[19] loss: 18693.1113\n",
      "[20] loss: 18661.4141\n",
      "Elapsed time: 657.59 seconds\n",
      "Working on 10.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22219.7656\n",
      "[2] loss: 20472.3516\n",
      "[3] loss: 20409.6602\n",
      "[4] loss: 20070.5605\n",
      "[5] loss: 19884.6758\n",
      "[6] loss: 19768.2285\n",
      "[7] loss: 19616.5898\n",
      "[8] loss: 19513.3633\n",
      "[9] loss: 19461.9980\n",
      "[10] loss: 19408.0430\n",
      "[11] loss: 19352.0117\n",
      "[12] loss: 19298.4883\n",
      "[13] loss: 19253.3281\n",
      "[14] loss: 19197.9727\n",
      "[15] loss: 19019.0020\n",
      "[16] loss: 18934.6172\n",
      "[17] loss: 18887.6699\n",
      "[18] loss: 18838.1992\n",
      "[19] loss: 18793.1055\n",
      "[20] loss: 18758.7637\n",
      "Elapsed time: 660.25 seconds\n",
      "Working on 11.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21859.3320\n",
      "[2] loss: 19650.5781\n",
      "[3] loss: 19503.4414\n",
      "[4] loss: 19415.5449\n",
      "[5] loss: 19345.5098\n",
      "[6] loss: 19299.0586\n",
      "[7] loss: 19266.6348\n",
      "[8] loss: 19231.4766\n",
      "[9] loss: 19187.5977\n",
      "[10] loss: 19135.5898\n",
      "[11] loss: 19079.1777\n",
      "[12] loss: 19023.0547\n",
      "[13] loss: 18973.8672\n",
      "[14] loss: 18572.9609\n",
      "[15] loss: 18482.0781\n",
      "[16] loss: 18448.1914\n",
      "[17] loss: 18419.4609\n",
      "[18] loss: 18378.3750\n",
      "[19] loss: 18355.0566\n",
      "[20] loss: 18341.8242\n",
      "Elapsed time: 686.17 seconds\n",
      "Working on 12.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22044.4648\n",
      "[2] loss: 20168.7402\n",
      "[3] loss: 19754.4844\n",
      "[4] loss: 19544.9336\n",
      "[5] loss: 19465.7051\n",
      "[6] loss: 19411.4219\n",
      "[7] loss: 19361.5742\n",
      "[8] loss: 19316.3398\n",
      "[9] loss: 19273.1328\n",
      "[10] loss: 19230.6641\n",
      "[11] loss: 19186.3457\n",
      "[12] loss: 19143.2266\n",
      "[13] loss: 18882.5000\n",
      "[14] loss: 18849.6758\n",
      "[15] loss: 18798.4375\n",
      "[16] loss: 18750.3828\n",
      "[17] loss: 18711.2480\n",
      "[18] loss: 18675.2656\n",
      "[19] loss: 18633.6094\n",
      "[20] loss: 18593.3945\n",
      "Elapsed time: 684.93 seconds\n",
      "Working on 13.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22425.0059\n",
      "[2] loss: 21850.0098\n",
      "[3] loss: 20600.2305\n",
      "[4] loss: 19783.9453\n",
      "[5] loss: 19623.4180\n",
      "[6] loss: 19541.3164\n",
      "[7] loss: 19498.5996\n",
      "[8] loss: 19407.9570\n",
      "[9] loss: 19284.8672\n",
      "[10] loss: 19198.2773\n",
      "[11] loss: 19040.0879\n",
      "[12] loss: 19027.3262\n",
      "[13] loss: 18919.9453\n",
      "[14] loss: 18870.6641\n",
      "[15] loss: 18837.2168\n",
      "[16] loss: 18780.5039\n",
      "[17] loss: 18761.9922\n",
      "[18] loss: 18740.4805\n",
      "[19] loss: 18754.7480\n",
      "[20] loss: 18720.3789\n",
      "Elapsed time: 675.21 seconds\n",
      "Working on 14.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22294.8926\n",
      "[2] loss: 20591.5391\n",
      "[3] loss: 20047.0547\n",
      "[4] loss: 19816.8711\n",
      "[5] loss: 19935.4941\n",
      "[6] loss: 19772.3184\n",
      "[7] loss: 19367.7812\n",
      "[8] loss: 19263.4727\n",
      "[9] loss: 19206.0625\n",
      "[10] loss: 19123.9219\n",
      "[11] loss: 19057.9980\n",
      "[12] loss: 19002.7930\n",
      "[13] loss: 18955.6738\n",
      "[14] loss: 18907.5938\n",
      "[15] loss: 18867.9258\n",
      "[16] loss: 18801.3125\n",
      "[17] loss: 18600.8750\n",
      "[18] loss: 18570.1211\n",
      "[19] loss: 18540.8164\n",
      "[20] loss: 18515.8320\n",
      "Elapsed time: 667.54 seconds\n",
      "Working on 15.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22088.9570\n",
      "[2] loss: 20642.3301\n",
      "[3] loss: 20436.7734\n",
      "[4] loss: 20460.5859\n",
      "[5] loss: 20316.2148\n",
      "[6] loss: 20197.7656\n",
      "[7] loss: 20093.6445\n",
      "[8] loss: 19994.6973\n",
      "[9] loss: 19924.6191\n",
      "[10] loss: 19902.7422\n",
      "[11] loss: 19799.1094\n",
      "[12] loss: 19571.8984\n",
      "[13] loss: 19516.1992\n",
      "[14] loss: 19406.6699\n",
      "[15] loss: 19316.0977\n",
      "[16] loss: 19243.5859\n",
      "[17] loss: 19151.4883\n",
      "[18] loss: 18691.9727\n",
      "[19] loss: 18572.1289\n",
      "[20] loss: 18519.1797\n",
      "Elapsed time: 678.21 seconds\n",
      "Working on 16.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22172.1055\n",
      "[2] loss: 21113.8906\n",
      "[3] loss: 20195.4844\n",
      "[4] loss: 20083.4727\n",
      "[5] loss: 19985.5938\n",
      "[6] loss: 19881.4336\n",
      "[7] loss: 19815.1504\n",
      "[8] loss: 19756.7031\n",
      "[9] loss: 19711.3496\n",
      "[10] loss: 19667.1582\n",
      "[11] loss: 19610.9961\n",
      "[12] loss: 19426.5547\n",
      "[13] loss: 19362.8594\n",
      "[14] loss: 19286.4180\n",
      "[15] loss: 18903.6914\n",
      "[16] loss: 18778.2402\n",
      "[17] loss: 18494.1055\n",
      "[18] loss: 18397.6602\n",
      "[19] loss: 18320.1953\n",
      "[20] loss: 18275.5977\n",
      "Elapsed time: 669.91 seconds\n",
      "Working on 17.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22303.2734\n",
      "[2] loss: 19923.7617\n",
      "[3] loss: 20053.0078\n",
      "[4] loss: 20008.7695\n",
      "[5] loss: 19857.1777\n",
      "[6] loss: 19707.6680\n",
      "[7] loss: 19440.4082\n",
      "[8] loss: 19315.8867\n",
      "[9] loss: 19283.2188\n",
      "[10] loss: 19245.6816\n",
      "[11] loss: 19085.8672\n",
      "[12] loss: 18891.3086\n",
      "[13] loss: 18862.9102\n",
      "[14] loss: 18705.0703\n",
      "[15] loss: 18333.3438\n",
      "[16] loss: 18296.2324\n",
      "[17] loss: 18260.1250\n",
      "[18] loss: 18278.1074\n",
      "[19] loss: 18184.7559\n",
      "[20] loss: 18152.3984\n",
      "Elapsed time: 671.71 seconds\n",
      "Working on 18.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20481.7148\n",
      "[2] loss: 20139.8750\n",
      "[3] loss: 19823.6953\n",
      "[4] loss: 19781.6992\n",
      "[5] loss: 19649.7520\n",
      "[6] loss: 19460.5293\n",
      "[7] loss: 19386.2070\n",
      "[8] loss: 19208.4570\n",
      "[9] loss: 19172.8594\n",
      "[10] loss: 19079.8008\n",
      "[11] loss: 19036.3691\n",
      "[12] loss: 18960.6406\n",
      "[13] loss: 18908.8633\n",
      "[14] loss: 18855.5039\n",
      "[15] loss: 18813.4805\n",
      "[16] loss: 18788.4648\n",
      "[17] loss: 18756.7070\n",
      "[18] loss: 18730.6875\n",
      "[19] loss: 18724.3633\n",
      "[20] loss: 18711.4258\n",
      "Elapsed time: 671.07 seconds\n",
      "Working on 19.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22279.0742\n",
      "[2] loss: 20676.7480\n",
      "[3] loss: 20494.9648\n",
      "[4] loss: 20254.5586\n",
      "[5] loss: 20049.2227\n",
      "[6] loss: 20006.2070\n",
      "[7] loss: 19952.1250\n",
      "[8] loss: 19885.6465\n",
      "[9] loss: 19839.7422\n",
      "[10] loss: 19801.5391\n",
      "[11] loss: 19782.6191\n",
      "[12] loss: 19747.6113\n",
      "[13] loss: 19702.4629\n",
      "[14] loss: 19467.3242\n",
      "[15] loss: 19435.9375\n",
      "[16] loss: 19411.6836\n",
      "[17] loss: 19373.6230\n",
      "[18] loss: 19363.6387\n",
      "[19] loss: 19349.0703\n",
      "[20] loss: 19331.9531\n",
      "Elapsed time: 674.70 seconds\n",
      "Working on 2.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20993.5254\n",
      "[2] loss: 20218.7070\n",
      "[3] loss: 20000.1836\n",
      "[4] loss: 19819.6055\n",
      "[5] loss: 19704.5977\n",
      "[6] loss: 19647.3008\n",
      "[7] loss: 19576.8652\n",
      "[8] loss: 19520.3789\n",
      "[9] loss: 19467.3535\n",
      "[10] loss: 19422.7891\n",
      "[11] loss: 19181.0996\n",
      "[12] loss: 19134.4766\n",
      "[13] loss: 19085.4297\n",
      "[14] loss: 19047.9180\n",
      "[15] loss: 19009.3750\n",
      "[16] loss: 18977.4336\n",
      "[17] loss: 18947.7578\n",
      "[18] loss: 18661.9746\n",
      "[19] loss: 18634.1367\n",
      "[20] loss: 18593.5430\n",
      "Elapsed time: 670.15 seconds\n",
      "Working on 20.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20776.1289\n",
      "[2] loss: 20728.9883\n",
      "[3] loss: 20527.3301\n",
      "[4] loss: 20283.3203\n",
      "[5] loss: 20098.4531\n",
      "[6] loss: 19977.3691\n",
      "[7] loss: 19849.8359\n",
      "[8] loss: 19591.9141\n",
      "[9] loss: 19511.6602\n",
      "[10] loss: 19446.5332\n",
      "[11] loss: 19247.4336\n",
      "[12] loss: 19217.3203\n",
      "[13] loss: 19017.2070\n",
      "[14] loss: 18835.7617\n",
      "[15] loss: 18746.4941\n",
      "[16] loss: 18170.2305\n",
      "[17] loss: 18090.9414\n",
      "[18] loss: 18029.9961\n",
      "[19] loss: 17951.4297\n",
      "[20] loss: 17906.1621\n",
      "Elapsed time: 664.87 seconds\n",
      "Working on 21.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22349.4004\n",
      "[2] loss: 20151.3418\n",
      "[3] loss: 19659.9141\n",
      "[4] loss: 19483.2070\n",
      "[5] loss: 19309.1484\n",
      "[6] loss: 19194.9844\n",
      "[7] loss: 19112.7344\n",
      "[8] loss: 19047.2461\n",
      "[9] loss: 18985.4355\n",
      "[10] loss: 18913.8438\n",
      "[11] loss: 18849.4863\n",
      "[12] loss: 18625.6953\n",
      "[13] loss: 18599.3301\n",
      "[14] loss: 18512.9805\n",
      "[15] loss: 18481.7812\n",
      "[16] loss: 18463.1680\n",
      "[17] loss: 18423.2422\n",
      "[18] loss: 18404.9863\n",
      "[19] loss: 18392.7441\n",
      "[20] loss: 18381.9648\n",
      "Elapsed time: 665.71 seconds\n",
      "Working on 22.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22281.5742\n",
      "[2] loss: 21603.0527\n",
      "[3] loss: 20167.6113\n",
      "[4] loss: 19712.0215\n",
      "[5] loss: 19619.3027\n",
      "[6] loss: 19490.1953\n",
      "[7] loss: 19392.6289\n",
      "[8] loss: 19300.4844\n",
      "[9] loss: 19139.0176\n",
      "[10] loss: 19073.6133\n",
      "[11] loss: 19039.2266\n",
      "[12] loss: 18978.2305\n",
      "[13] loss: 18934.5508\n",
      "[14] loss: 18901.8984\n",
      "[15] loss: 18880.0449\n",
      "[16] loss: 18848.9570\n",
      "[17] loss: 18826.3086\n",
      "[18] loss: 18812.8906\n",
      "[19] loss: 18806.6367\n",
      "[20] loss: 18806.6602\n",
      "Elapsed time: 666.29 seconds\n",
      "Working on 23.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22325.9668\n",
      "[2] loss: 21036.4414\n",
      "[3] loss: 20586.6484\n",
      "[4] loss: 20559.7129\n",
      "[5] loss: 20314.2656\n",
      "[6] loss: 20101.5977\n",
      "[7] loss: 20051.2852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] loss: 20037.1738\n",
      "[9] loss: 19931.9883\n",
      "[10] loss: 19829.9824\n",
      "[11] loss: 19648.5000\n",
      "[12] loss: 19378.9648\n",
      "[13] loss: 19301.1406\n",
      "[14] loss: 19234.5820\n",
      "[15] loss: 19164.6211\n",
      "[16] loss: 19109.8340\n",
      "[17] loss: 19038.2891\n",
      "[18] loss: 18952.4297\n",
      "[19] loss: 18873.4473\n",
      "[20] loss: 18787.8926\n",
      "Elapsed time: 661.41 seconds\n",
      "Working on 24.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22424.5273\n",
      "[2] loss: 20913.1953\n",
      "[3] loss: 20950.1641\n",
      "[4] loss: 20849.5312\n",
      "[5] loss: 20658.4336\n",
      "[6] loss: 20453.8906\n",
      "[7] loss: 20288.4023\n",
      "[8] loss: 20158.9375\n",
      "[9] loss: 19942.1602\n",
      "[10] loss: 19537.1973\n",
      "[11] loss: 19824.3848\n",
      "[12] loss: 19768.3438\n",
      "[13] loss: 19749.3418\n",
      "[14] loss: 19746.6504\n",
      "[15] loss: 19707.8828\n",
      "[16] loss: 19656.8281\n",
      "[17] loss: 19495.8613\n",
      "[18] loss: 19461.1895\n",
      "[19] loss: 19425.6582\n",
      "[20] loss: 19016.6250\n",
      "Elapsed time: 663.84 seconds\n",
      "Working on 25.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21276.6660\n",
      "[2] loss: 20188.5430\n",
      "[3] loss: 19860.6973\n",
      "[4] loss: 19681.5156\n",
      "[5] loss: 19611.6602\n",
      "[6] loss: 19543.8984\n",
      "[7] loss: 19468.6426\n",
      "[8] loss: 19394.8555\n",
      "[9] loss: 19335.9062\n",
      "[10] loss: 19295.2168\n",
      "[11] loss: 19254.4062\n",
      "[12] loss: 19206.2051\n",
      "[13] loss: 19155.5469\n",
      "[14] loss: 19100.0625\n",
      "[15] loss: 18917.4258\n",
      "[16] loss: 18783.7969\n",
      "[17] loss: 18521.6055\n",
      "[18] loss: 18467.5820\n",
      "[19] loss: 18418.0234\n",
      "[20] loss: 18364.1758\n",
      "Elapsed time: 664.34 seconds\n",
      "Working on 26.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22379.4414\n",
      "[2] loss: 21096.2969\n",
      "[3] loss: 20733.9922\n",
      "[4] loss: 20860.0156\n",
      "[5] loss: 20773.0078\n",
      "[6] loss: 20796.5898\n",
      "[7] loss: 20762.2109\n",
      "[8] loss: 20699.0352\n",
      "[9] loss: 20643.1914\n",
      "[10] loss: 20583.0977\n",
      "[11] loss: 20064.6484\n",
      "[12] loss: 19869.2734\n",
      "[13] loss: 19775.7871\n",
      "[14] loss: 19686.0723\n",
      "[15] loss: 19631.1406\n",
      "[16] loss: 19616.3574\n",
      "[17] loss: 19520.5938\n",
      "[18] loss: 19483.1328\n",
      "[19] loss: 19199.9785\n",
      "[20] loss: 19098.2051\n",
      "Elapsed time: 663.20 seconds\n",
      "Working on 27.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21674.5742\n",
      "[2] loss: 20780.7188\n",
      "[3] loss: 20333.9766\n",
      "[4] loss: 20156.9746\n",
      "[5] loss: 20035.8301\n",
      "[6] loss: 19938.8438\n",
      "[7] loss: 19864.9512\n",
      "[8] loss: 19792.8262\n",
      "[9] loss: 19712.6367\n",
      "[10] loss: 19650.9844\n",
      "[11] loss: 19607.6055\n",
      "[12] loss: 19570.3984\n",
      "[13] loss: 19523.4297\n",
      "[14] loss: 19473.9082\n",
      "[15] loss: 19437.6094\n",
      "[16] loss: 19433.2617\n",
      "[17] loss: 19389.0391\n",
      "[18] loss: 19238.4961\n",
      "[19] loss: 19218.9023\n",
      "[20] loss: 19205.0527\n",
      "Elapsed time: 648.06 seconds\n",
      "Working on 28.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22404.0430\n",
      "[2] loss: 20870.9258\n",
      "[3] loss: 21367.5352\n",
      "[4] loss: 21000.2812\n",
      "[5] loss: 21044.8203\n",
      "[6] loss: 20882.9922\n",
      "[7] loss: 20710.9648\n",
      "[8] loss: 20675.4180\n",
      "[9] loss: 20580.7734\n",
      "[10] loss: 20372.0879\n",
      "[11] loss: 20091.4570\n",
      "[12] loss: 20035.3555\n",
      "[13] loss: 19975.3359\n",
      "[14] loss: 19919.0742\n",
      "[15] loss: 19859.1953\n",
      "[16] loss: 19100.1602\n",
      "[17] loss: 18695.4043\n",
      "[18] loss: 18622.0059\n",
      "[19] loss: 18218.9961\n",
      "[20] loss: 18142.6758\n",
      "Elapsed time: 652.33 seconds\n",
      "Working on 29.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22131.4961\n",
      "[2] loss: 21257.6660\n",
      "[3] loss: 20649.0684\n",
      "[4] loss: 20491.6875\n",
      "[5] loss: 20438.9453\n",
      "[6] loss: 20307.7070\n",
      "[7] loss: 20334.7578\n",
      "[8] loss: 20118.0508\n",
      "[9] loss: 20079.5742\n",
      "[10] loss: 20252.5059\n",
      "[11] loss: 20241.0977\n",
      "[12] loss: 19925.7852\n",
      "[13] loss: 19874.7734\n",
      "[14] loss: 19841.4453\n",
      "[15] loss: 19776.3594\n",
      "[16] loss: 19676.2773\n",
      "[17] loss: 19546.2891\n",
      "[18] loss: 18981.7246\n",
      "[19] loss: 18746.0195\n",
      "[20] loss: 19407.1211\n",
      "Elapsed time: 657.76 seconds\n",
      "Working on 3.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21632.7578\n",
      "[2] loss: 20252.2461\n",
      "[3] loss: 19655.6719\n",
      "[4] loss: 19488.8711\n",
      "[5] loss: 19338.8711\n",
      "[6] loss: 19277.1680\n",
      "[7] loss: 19248.6191\n",
      "[8] loss: 19181.4688\n",
      "[9] loss: 19135.1602\n",
      "[10] loss: 19095.1953\n",
      "[11] loss: 19047.5156\n",
      "[12] loss: 19003.2891\n",
      "[13] loss: 18959.1680\n",
      "[14] loss: 18922.8125\n",
      "[15] loss: 18878.4023\n",
      "[16] loss: 18825.7031\n",
      "[17] loss: 18777.4180\n",
      "[18] loss: 18731.6562\n",
      "[19] loss: 18727.2910\n",
      "[20] loss: 18794.1758\n",
      "Elapsed time: 661.18 seconds\n",
      "Working on 4.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21751.1562\n",
      "[2] loss: 20246.1914\n",
      "[3] loss: 19897.7188\n",
      "[4] loss: 19693.2812\n",
      "[5] loss: 19547.1836\n",
      "[6] loss: 19445.2363\n",
      "[7] loss: 19366.1816\n",
      "[8] loss: 19271.3164\n",
      "[9] loss: 19181.9102\n",
      "[10] loss: 19102.7227\n",
      "[11] loss: 19041.0859\n",
      "[12] loss: 18994.7930\n",
      "[13] loss: 18663.0410\n",
      "[14] loss: 18625.1387\n",
      "[15] loss: 18591.4805\n",
      "[16] loss: 18571.3516\n",
      "[17] loss: 18535.9062\n",
      "[18] loss: 18502.1133\n",
      "[19] loss: 18472.9414\n",
      "[20] loss: 18456.3359\n",
      "Elapsed time: 659.58 seconds\n",
      "Working on 5.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21311.2773\n",
      "[2] loss: 19962.8555\n",
      "[3] loss: 19669.0078\n",
      "[4] loss: 19538.9844\n",
      "[5] loss: 19448.7734\n",
      "[6] loss: 19399.2773\n",
      "[7] loss: 19326.3145\n",
      "[8] loss: 19286.7969\n",
      "[9] loss: 19242.8359\n",
      "[10] loss: 19197.2148\n",
      "[11] loss: 19146.8398\n",
      "[12] loss: 19103.7656\n",
      "[13] loss: 19059.1602\n",
      "[14] loss: 18910.6445\n",
      "[15] loss: 18644.6445\n",
      "[16] loss: 18635.7422\n",
      "[17] loss: 18607.3750\n",
      "[18] loss: 18582.5762\n",
      "[19] loss: 18562.6328\n",
      "[20] loss: 18528.7578\n",
      "Elapsed time: 658.18 seconds\n",
      "Working on 6.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20726.5020\n",
      "[2] loss: 20374.0977\n",
      "[3] loss: 19993.0352\n",
      "[4] loss: 19798.9355\n",
      "[5] loss: 19652.2129\n",
      "[6] loss: 19531.5234\n",
      "[7] loss: 19328.1484\n",
      "[8] loss: 19192.6191\n",
      "[9] loss: 19115.7344\n",
      "[10] loss: 18847.5723\n",
      "[11] loss: 18622.7461\n",
      "[12] loss: 18526.4004\n",
      "[13] loss: 18461.3750\n",
      "[14] loss: 18423.3730\n",
      "[15] loss: 18335.5586\n",
      "[16] loss: 17960.3203\n",
      "[17] loss: 17944.0898\n",
      "[18] loss: 17857.1914\n",
      "[19] loss: 17858.3086\n",
      "[20] loss: 17827.9316\n",
      "Elapsed time: 668.86 seconds\n",
      "Working on 7.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21888.4219\n",
      "[2] loss: 20721.1016\n",
      "[3] loss: 20243.7305\n",
      "[4] loss: 20016.6680\n",
      "[5] loss: 19871.6523\n",
      "[6] loss: 19742.1582\n",
      "[7] loss: 19704.7695\n",
      "[8] loss: 19951.4805\n",
      "[9] loss: 19986.2305\n",
      "[10] loss: 19938.6309\n",
      "[11] loss: 19790.3164\n",
      "[12] loss: 19695.1133\n",
      "[13] loss: 19543.4473\n",
      "[14] loss: 19542.2070\n",
      "[15] loss: 19390.4648\n",
      "[16] loss: 19053.8281\n",
      "[17] loss: 19011.0117\n",
      "[18] loss: 18963.8398\n",
      "[19] loss: 18923.2773\n",
      "[20] loss: 18879.1172\n",
      "Elapsed time: 661.56 seconds\n",
      "Working on 8.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22182.9414\n",
      "[2] loss: 21042.9609\n",
      "[3] loss: 20323.3438\n",
      "[4] loss: 20276.9453\n",
      "[5] loss: 20099.8457\n",
      "[6] loss: 20015.2441\n",
      "[7] loss: 20259.5137\n",
      "[8] loss: 20204.6016\n",
      "[9] loss: 20132.6816\n",
      "[10] loss: 20064.8086\n",
      "[11] loss: 20002.6719\n",
      "[12] loss: 19863.8125\n",
      "[13] loss: 19594.1328\n",
      "[14] loss: 19412.5234\n",
      "[15] loss: 19379.0879\n",
      "[16] loss: 19320.8711\n",
      "[17] loss: 19197.4766\n",
      "[18] loss: 19028.0586\n",
      "[19] loss: 18914.6016\n",
      "[20] loss: 18861.6445\n",
      "Elapsed time: 663.22 seconds\n",
      "Working on 9.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22165.8203\n",
      "[2] loss: 20710.9297\n",
      "[3] loss: 20482.8906\n",
      "[4] loss: 20377.3164\n",
      "[5] loss: 20344.5723\n",
      "[6] loss: 20260.2891\n",
      "[7] loss: 20135.2930\n",
      "[8] loss: 19902.3359\n",
      "[9] loss: 19972.8750\n",
      "[10] loss: 19918.2266\n",
      "[11] loss: 19862.6719\n",
      "[12] loss: 19808.5801\n",
      "[13] loss: 19589.0293\n",
      "[14] loss: 19503.4570\n",
      "[15] loss: 19460.4258\n",
      "[16] loss: 19425.7891\n",
      "[17] loss: 18992.9297\n",
      "[18] loss: 18908.4766\n",
      "[19] loss: 18856.3164\n",
      "[20] loss: 18847.9473\n",
      "Elapsed time: 658.91 seconds\n",
      "Top-1% accuracy: 0.25±0.13\n",
      "Top-5% accuracy: 0.19±0.09\n",
      "Top-10% accuracy: 0.21±0.07\n",
      "Kendall tau distance: 0.00±0.00\n",
      "Running time: 665.10±9.07\n"
     ]
    }
   ],
   "source": [
    "# Result that makes GRUCell bias=True\n",
    "top1_list = [\n",
    "top5_list = []\n",
    "top10_list = []\n",
    "ken_list = []\n",
    "elapsed_time_list = []\n",
    "for filename in filenames[:-1]:\n",
    "    top1,top5,top10,ken,elapsed_time = train_and_result(filename)\n",
    "    top1_list.append(top1)\n",
    "    top5_list.append(top5)\n",
    "    top10_list.append(top10)\n",
    "    ken_list.append(ken)\n",
    "    elapsed_time_list.append(elapsed_time)\n",
    "# Calculate Mean for each evaluation metrics:\n",
    "print(\"Top-1% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top1_list)),np.std(np.array(top1_list))))\n",
    "print(\"Top-5% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top5_list)),np.std(np.array(top5_list))))\n",
    "print(\"Top-10% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top10_list)),np.std(np.array(top10_list))))\n",
    "print(\"Kendall tau distance: {:.2f}±{:.2f}\".format(np.mean(np.array(ken_list)),np.std(np.array(ken_list))))\n",
    "print(\"Running time: {:.2f}±{:.2f}\".format(np.mean(np.array(elapsed_time_list)),np.std(np.array(elapsed_time_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "178487b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_result_for_once(filename):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    k = [1,5,10]\n",
    "    \n",
    "    print(\"Working on {}\".format(filename))\n",
    "    # Prepare the data\n",
    "    G = gList[filename]['graph']\n",
    "    y = torch.tensor([list(gList[filename]['score'].values())])\n",
    "    y = torch.transpose(y,0,1)\n",
    "\n",
    "    # Define the models\n",
    "    input_size = 3\n",
    "    hidden_size = 128\n",
    "    output_size = 1\n",
    "    num_layers = 5\n",
    "    encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "    decoder = DrBCDecoder(hidden_size,hidden_size,output_size)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    model = EncoderDecoder(encoder,decoder)\n",
    "    model.to(device)\n",
    "    \n",
    "    n = G.number_of_nodes()\n",
    "    num_episodes = 1000\n",
    "    lr = 0.001\n",
    "    sample_qty = 5*n\n",
    "\n",
    "    # Define the loss and optimizer\n",
    "    criterion = nn.BCELoss(reduction = 'sum')\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction = 'sum') # Because only pred will go through sigmoid, but ground truth won't\n",
    "    #optimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get the inputs\n",
    "    inputs = gen_nodes_feature(G)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Start training on {}\".format(device))\n",
    "    pairs = sampling(0,n-1,sample_qty)\n",
    "    for episode in range(num_episodes):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # model\n",
    "        outputs = model(inputs)\n",
    "        pred,gt = bc_pairs(pairs,outputs,y)\n",
    "        loss = criterion(pred,gt)\n",
    "        if ~loss.requires_grad:\n",
    "            loss.requires_grad_()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if best_loss>loss:\n",
    "            best_loss = loss\n",
    "            best_out = outputs\n",
    "            #best_model_weights = model.state_dict()\n",
    "            \n",
    "        # Print statistics\n",
    "        #if episode%5 == 4:\n",
    "        print('[%d] loss: %.4f' %(episode + 1, loss.item()))\n",
    "            \n",
    "    #torch.save(best_model_weights, 'best_model.pth')\n",
    "    top1 = topN(1,best_out,y)\n",
    "    top5 = topN(5,best_out,y)\n",
    "    top10 = topN(10,best_out,y)\n",
    "    ken = kendall(best_out,y)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "    return top1,top5,top10,ken,elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edf3b9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22461.7969\n",
      "[2] loss: 20742.7891\n",
      "[3] loss: 20246.5625\n",
      "[4] loss: 20161.9609\n",
      "[5] loss: 20254.7402\n",
      "[6] loss: 19816.6562\n",
      "[7] loss: 19404.4980\n",
      "[8] loss: 19352.0859\n",
      "[9] loss: 19308.7871\n",
      "[10] loss: 19253.8906\n",
      "[11] loss: 19193.5547\n",
      "[12] loss: 19104.4707\n",
      "[13] loss: 18722.1914\n",
      "[14] loss: 18656.6934\n",
      "[15] loss: 18579.1406\n",
      "[16] loss: 18316.6309\n",
      "[17] loss: 18534.3594\n",
      "[18] loss: 18215.4082\n",
      "[19] loss: 18184.3848\n",
      "[20] loss: 18089.5410\n",
      "[21] loss: 18098.9336\n",
      "[22] loss: 18074.1934\n",
      "[23] loss: 18077.3086\n",
      "[24] loss: 18045.0195\n",
      "[25] loss: 18038.3320\n",
      "[26] loss: 18029.1289\n",
      "[27] loss: 18016.5156\n",
      "[28] loss: 18002.7676\n",
      "[29] loss: 18008.0586\n",
      "[30] loss: 17999.2070\n",
      "[31] loss: 17992.0293\n",
      "[32] loss: 17987.0703\n",
      "[33] loss: 17984.0703\n",
      "[34] loss: 17984.5957\n",
      "[35] loss: 17980.7598\n",
      "[36] loss: 17977.6953\n",
      "[37] loss: 17974.8809\n",
      "[38] loss: 17973.9180\n",
      "[39] loss: 17974.1562\n",
      "[40] loss: 17972.6055\n",
      "[41] loss: 17970.8848\n",
      "[42] loss: 17969.5879\n",
      "[43] loss: 17968.3984\n",
      "[44] loss: 17968.2207\n",
      "[45] loss: 17966.7422\n",
      "[46] loss: 17965.1094\n",
      "[47] loss: 17963.7539\n",
      "[48] loss: 17962.9062\n",
      "[49] loss: 17962.6230\n",
      "[50] loss: 17962.3281\n",
      "[51] loss: 17962.2969\n",
      "[52] loss: 17961.2031\n",
      "[53] loss: 17960.0332\n",
      "[54] loss: 17959.2734\n",
      "[55] loss: 17958.9219\n",
      "[56] loss: 17958.8125\n",
      "[57] loss: 17958.7148\n",
      "[58] loss: 17959.1016\n",
      "[59] loss: 17958.4336\n",
      "[60] loss: 17957.5918\n",
      "[61] loss: 17956.9902\n",
      "[62] loss: 17956.6172\n",
      "[63] loss: 17956.3125\n",
      "[64] loss: 17956.3105\n",
      "[65] loss: 17956.5234\n",
      "[66] loss: 17956.5898\n",
      "[67] loss: 17956.0078\n",
      "[68] loss: 17955.5430\n",
      "[69] loss: 17955.2031\n",
      "[70] loss: 17955.0664\n",
      "[71] loss: 17954.9941\n",
      "[72] loss: 17954.9238\n",
      "[73] loss: 17954.9375\n",
      "[74] loss: 17954.4375\n",
      "[75] loss: 17954.0723\n",
      "[76] loss: 17953.8926\n",
      "[77] loss: 17953.8691\n",
      "[78] loss: 17953.8984\n",
      "[79] loss: 17953.6387\n",
      "[80] loss: 17953.4297\n",
      "[81] loss: 17953.3320\n",
      "[82] loss: 17953.3320\n",
      "[83] loss: 17953.4824\n",
      "[84] loss: 17953.2578\n",
      "[85] loss: 17952.9785\n",
      "[86] loss: 17952.8203\n",
      "[87] loss: 17952.7461\n",
      "[88] loss: 17952.7773\n",
      "[89] loss: 17952.9102\n",
      "[90] loss: 17952.6250\n",
      "[91] loss: 17952.4141\n",
      "[92] loss: 17952.3281\n",
      "[93] loss: 17952.3418\n",
      "[94] loss: 17952.2969\n",
      "[95] loss: 17952.0723\n",
      "[96] loss: 17951.9492\n",
      "[97] loss: 17951.9375\n",
      "[98] loss: 17952.0117\n",
      "[99] loss: 17951.9434\n",
      "[100] loss: 17951.8008\n",
      "[101] loss: 17951.7500\n",
      "[102] loss: 17951.7656\n",
      "[103] loss: 17951.7344\n",
      "[104] loss: 17951.6094\n",
      "[105] loss: 17951.5664\n",
      "[106] loss: 17951.5742\n",
      "[107] loss: 17951.4902\n",
      "[108] loss: 17951.3926\n",
      "[109] loss: 17951.3672\n",
      "[110] loss: 17951.4062\n",
      "[111] loss: 17951.2168\n",
      "[112] loss: 17951.1367\n",
      "[113] loss: 17951.1562\n",
      "[114] loss: 17951.2383\n",
      "[115] loss: 17951.0234\n",
      "[116] loss: 17950.9180\n",
      "[117] loss: 17950.9219\n",
      "[118] loss: 17951.0352\n",
      "[119] loss: 17950.9004\n",
      "[120] loss: 17950.7988\n",
      "[121] loss: 17950.7812\n",
      "[122] loss: 17950.8633\n",
      "[123] loss: 17950.7734\n",
      "[124] loss: 17950.7266\n",
      "[125] loss: 17950.7344\n",
      "[126] loss: 17950.6133\n",
      "[127] loss: 17950.5684\n",
      "[128] loss: 17950.6094\n",
      "[129] loss: 17950.3984\n",
      "[130] loss: 17950.3145\n",
      "[131] loss: 17950.3379\n",
      "[132] loss: 17950.4629\n",
      "[133] loss: 17950.4785\n",
      "[134] loss: 17950.3242\n",
      "[135] loss: 17950.2500\n",
      "[136] loss: 17950.2578\n",
      "[137] loss: 17950.3164\n",
      "[138] loss: 17950.1523\n",
      "[139] loss: 17950.0996\n",
      "[140] loss: 17950.1367\n",
      "[141] loss: 17950.1406\n",
      "[142] loss: 17950.0195\n",
      "[143] loss: 17949.9902\n",
      "[144] loss: 17950.0352\n",
      "[145] loss: 17950.1172\n",
      "[146] loss: 17950.0215\n",
      "[147] loss: 17949.9707\n",
      "[148] loss: 17949.9688\n",
      "[149] loss: 17949.8828\n",
      "[150] loss: 17949.8633\n",
      "[151] loss: 17949.8945\n",
      "[152] loss: 17949.7852\n",
      "[153] loss: 17949.7578\n",
      "[154] loss: 17949.8125\n",
      "[155] loss: 17949.7910\n",
      "[156] loss: 17949.6914\n",
      "[157] loss: 17949.6719\n",
      "[158] loss: 17949.7207\n",
      "[159] loss: 17949.6465\n",
      "[160] loss: 17949.6309\n",
      "[161] loss: 17949.6562\n",
      "[162] loss: 17949.5430\n",
      "[163] loss: 17949.5098\n",
      "[164] loss: 17949.5586\n",
      "[165] loss: 17949.5547\n",
      "[166] loss: 17949.4551\n",
      "[167] loss: 17949.4414\n",
      "[168] loss: 17949.5059\n",
      "[169] loss: 17949.4297\n",
      "[170] loss: 17949.3926\n",
      "[171] loss: 17949.4141\n",
      "[172] loss: 17949.3477\n",
      "[173] loss: 17949.3398\n",
      "[174] loss: 17949.3730\n",
      "[175] loss: 17949.2461\n",
      "[176] loss: 17949.2246\n",
      "[177] loss: 17949.3027\n",
      "[178] loss: 17949.3828\n",
      "[179] loss: 17949.1035\n",
      "[180] loss: 17948.9609\n",
      "[181] loss: 17948.9219\n",
      "[182] loss: 17948.9805\n",
      "[183] loss: 17949.1094\n",
      "[184] loss: 17949.2734\n",
      "[185] loss: 17949.4238\n",
      "[186] loss: 17949.0371\n",
      "[187] loss: 17948.7812\n",
      "[188] loss: 17948.6680\n",
      "[189] loss: 17948.6660\n",
      "[190] loss: 17948.7695\n",
      "[191] loss: 17948.9590\n",
      "[192] loss: 17949.1738\n",
      "[193] loss: 17949.3750\n",
      "[194] loss: 17948.9961\n",
      "[195] loss: 17948.7383\n",
      "[196] loss: 17948.5957\n",
      "[197] loss: 17948.5664\n",
      "[198] loss: 17948.6328\n",
      "[199] loss: 17948.7852\n",
      "[200] loss: 17948.9922\n",
      "[201] loss: 17949.2539\n",
      "[202] loss: 17948.8477\n",
      "[203] loss: 17948.5762\n",
      "[204] loss: 17948.4219\n",
      "[205] loss: 17948.3906\n",
      "[206] loss: 17948.4336\n",
      "[207] loss: 17948.5742\n",
      "[208] loss: 17948.7695\n",
      "[209] loss: 17948.9805\n",
      "[210] loss: 17949.1680\n",
      "[211] loss: 17948.7070\n",
      "[212] loss: 17948.3594\n",
      "[213] loss: 17948.1504\n",
      "[214] loss: 17948.0938\n",
      "[215] loss: 17948.1602\n",
      "[216] loss: 17948.3281\n",
      "[217] loss: 17948.5605\n",
      "[218] loss: 17948.7930\n",
      "[219] loss: 17949.0039\n",
      "[220] loss: 17948.9746\n",
      "[221] loss: 17948.7812\n",
      "[222] loss: 17948.6406\n",
      "[223] loss: 17948.5703\n",
      "[224] loss: 17948.5742\n",
      "[225] loss: 17948.6270\n",
      "[226] loss: 17948.7305\n",
      "[227] loss: 17948.8262\n",
      "[228] loss: 17948.4766\n",
      "[229] loss: 17948.2305\n",
      "[230] loss: 17948.1016\n",
      "[231] loss: 17948.0840\n",
      "[232] loss: 17948.1699\n",
      "[233] loss: 17948.3164\n",
      "[234] loss: 17948.5156\n",
      "[235] loss: 17948.7188\n",
      "[236] loss: 17948.5586\n",
      "[237] loss: 17948.4883\n",
      "[238] loss: 17948.4492\n",
      "[239] loss: 17948.4570\n",
      "[240] loss: 17948.4980\n",
      "[241] loss: 17948.5391\n",
      "[242] loss: 17948.5703\n",
      "[243] loss: 17948.4023\n",
      "[244] loss: 17948.2812\n",
      "[245] loss: 17948.2500\n",
      "[246] loss: 17948.2891\n",
      "[247] loss: 17948.3789\n",
      "[248] loss: 17948.4766\n",
      "[249] loss: 17948.3887\n",
      "[250] loss: 17948.3359\n",
      "[251] loss: 17948.3242\n",
      "[252] loss: 17948.3438\n",
      "[253] loss: 17948.3848\n",
      "[254] loss: 17948.2969\n",
      "[255] loss: 17948.2715\n",
      "[256] loss: 17948.2969\n",
      "[257] loss: 17948.2285\n",
      "[258] loss: 17948.2363\n",
      "[259] loss: 17948.2969\n",
      "[260] loss: 17948.2852\n",
      "[261] loss: 17948.2559\n",
      "[262] loss: 17948.2500\n",
      "[263] loss: 17948.2676\n",
      "[264] loss: 17948.2109\n",
      "[265] loss: 17948.2090\n",
      "[266] loss: 17948.2500\n",
      "[267] loss: 17948.1758\n",
      "[268] loss: 17948.1641\n",
      "[269] loss: 17948.2012\n",
      "[270] loss: 17948.1875\n",
      "[271] loss: 17948.1172\n",
      "[272] loss: 17948.1172\n",
      "[273] loss: 17948.1680\n",
      "[274] loss: 17948.1445\n",
      "[275] loss: 17948.1445\n",
      "[276] loss: 17948.1445\n",
      "[277] loss: 17948.1172\n",
      "[278] loss: 17948.1133\n",
      "[279] loss: 17948.0781\n",
      "[280] loss: 17948.0879\n",
      "[281] loss: 17948.0742\n",
      "[282] loss: 17948.0742\n",
      "[283] loss: 17948.0684\n",
      "[284] loss: 17948.0527\n",
      "[285] loss: 17948.0664\n",
      "[286] loss: 17948.0215\n",
      "[287] loss: 17948.0273\n",
      "[288] loss: 17948.0527\n",
      "[289] loss: 17947.9648\n",
      "[290] loss: 17947.9648\n",
      "[291] loss: 17948.0430\n",
      "[292] loss: 17948.1055\n",
      "[293] loss: 17947.9297\n",
      "[294] loss: 17947.8320\n",
      "[295] loss: 17947.8125\n",
      "[296] loss: 17947.8633\n",
      "[297] loss: 17947.9727\n",
      "[298] loss: 17948.0938\n",
      "[299] loss: 17948.0859\n",
      "[300] loss: 17947.9727\n",
      "[301] loss: 17947.9062\n",
      "[302] loss: 17947.8965\n",
      "[303] loss: 17947.9336\n",
      "[304] loss: 17947.9922\n",
      "[305] loss: 17947.9922\n",
      "[306] loss: 17947.8809\n",
      "[307] loss: 17947.8223\n",
      "[308] loss: 17947.8281\n",
      "[309] loss: 17947.8887\n",
      "[310] loss: 17947.9570\n",
      "[311] loss: 17947.9375\n",
      "[312] loss: 17947.9180\n",
      "[313] loss: 17947.9062\n",
      "[314] loss: 17947.8691\n",
      "[315] loss: 17947.8555\n",
      "[316] loss: 17947.8750\n",
      "[317] loss: 17947.8633\n",
      "[318] loss: 17947.8457\n",
      "[319] loss: 17947.8516\n",
      "[320] loss: 17947.8242\n",
      "[321] loss: 17947.8320\n",
      "[322] loss: 17947.8477\n",
      "[323] loss: 17947.7402\n",
      "[324] loss: 17947.6914\n",
      "[325] loss: 17947.7109\n",
      "[326] loss: 17947.7773\n",
      "[327] loss: 17947.8789\n",
      "[328] loss: 17947.8203\n",
      "[329] loss: 17947.7852\n",
      "[330] loss: 17947.7832\n",
      "[331] loss: 17947.8008\n",
      "[332] loss: 17947.7773\n",
      "[333] loss: 17947.7754\n",
      "[334] loss: 17947.7891\n",
      "[335] loss: 17947.6641\n",
      "[336] loss: 17947.5938\n",
      "[337] loss: 17947.5879\n",
      "[338] loss: 17947.6367\n",
      "[339] loss: 17947.7246\n",
      "[340] loss: 17947.8320\n",
      "[341] loss: 17947.6602\n",
      "[342] loss: 17947.5547\n",
      "[343] loss: 17947.5254\n",
      "[344] loss: 17947.5586\n",
      "[345] loss: 17947.6328\n",
      "[346] loss: 17947.7207\n",
      "[347] loss: 17947.8184\n",
      "[348] loss: 17947.6328\n",
      "[349] loss: 17947.5059\n",
      "[350] loss: 17947.4609\n",
      "[351] loss: 17947.4785\n",
      "[352] loss: 17947.5293\n",
      "[353] loss: 17947.6230\n",
      "[354] loss: 17947.7227\n",
      "[355] loss: 17947.8086\n",
      "[356] loss: 17947.5117\n",
      "[357] loss: 17947.3008\n",
      "[358] loss: 17947.1875\n",
      "[359] loss: 17947.1582\n",
      "[360] loss: 17947.2031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[361] loss: 17947.3105\n",
      "[362] loss: 17947.4492\n",
      "[363] loss: 17947.6074\n",
      "[364] loss: 17947.7461\n",
      "[365] loss: 17947.8535\n",
      "[366] loss: 17947.7773\n",
      "[367] loss: 17947.7324\n",
      "[368] loss: 17947.7246\n",
      "[369] loss: 17947.7227\n",
      "[370] loss: 17947.6953\n",
      "[371] loss: 17947.6836\n",
      "[372] loss: 17947.6406\n",
      "[373] loss: 17947.6074\n",
      "[374] loss: 17947.5957\n",
      "[375] loss: 17947.6133\n",
      "[376] loss: 17947.6094\n",
      "[377] loss: 17947.5469\n",
      "[378] loss: 17947.5332\n",
      "[379] loss: 17947.5703\n",
      "[380] loss: 17947.6250\n",
      "[381] loss: 17947.5391\n",
      "[382] loss: 17947.4922\n",
      "[383] loss: 17947.4844\n",
      "[384] loss: 17947.5176\n",
      "[385] loss: 17947.5742\n",
      "[386] loss: 17947.5703\n",
      "[387] loss: 17947.5059\n",
      "[388] loss: 17947.4844\n",
      "[389] loss: 17947.4941\n",
      "[390] loss: 17947.5234\n",
      "[391] loss: 17947.5547\n",
      "[392] loss: 17947.4023\n",
      "[393] loss: 17947.3066\n",
      "[394] loss: 17947.2734\n",
      "[395] loss: 17947.2930\n",
      "[396] loss: 17947.3555\n",
      "[397] loss: 17947.4453\n",
      "[398] loss: 17947.5352\n",
      "[399] loss: 17947.5977\n",
      "[400] loss: 17947.4219\n",
      "[401] loss: 17947.2910\n",
      "[402] loss: 17947.2402\n",
      "[403] loss: 17947.2461\n",
      "[404] loss: 17947.2988\n",
      "[405] loss: 17947.3789\n",
      "[406] loss: 17947.4648\n",
      "[407] loss: 17947.5430\n",
      "[408] loss: 17947.4941\n",
      "[409] loss: 17947.4570\n",
      "[410] loss: 17947.4453\n",
      "[411] loss: 17947.4492\n",
      "[412] loss: 17947.4473\n",
      "[413] loss: 17947.4297\n",
      "[414] loss: 17947.4297\n",
      "[415] loss: 17947.4023\n",
      "[416] loss: 17947.3809\n",
      "[417] loss: 17947.3848\n",
      "[418] loss: 17947.4180\n",
      "[419] loss: 17947.4023\n",
      "[420] loss: 17947.3984\n",
      "[421] loss: 17947.4043\n",
      "[422] loss: 17947.3750\n",
      "[423] loss: 17947.3750\n",
      "[424] loss: 17947.3906\n",
      "[425] loss: 17947.3066\n",
      "[426] loss: 17947.2500\n",
      "[427] loss: 17947.2383\n",
      "[428] loss: 17947.2734\n",
      "[429] loss: 17947.3320\n",
      "[430] loss: 17947.4121\n",
      "[431] loss: 17947.3477\n",
      "[432] loss: 17947.3125\n",
      "[433] loss: 17947.3086\n",
      "[434] loss: 17947.3262\n",
      "[435] loss: 17947.3594\n",
      "[436] loss: 17947.3457\n",
      "[437] loss: 17947.3320\n",
      "[438] loss: 17947.3281\n",
      "[439] loss: 17947.3320\n",
      "[440] loss: 17947.2930\n",
      "[441] loss: 17947.2891\n",
      "[442] loss: 17947.3164\n",
      "[443] loss: 17947.3340\n",
      "[444] loss: 17947.2500\n",
      "[445] loss: 17947.2051\n",
      "[446] loss: 17947.2031\n",
      "[447] loss: 17947.2305\n",
      "[448] loss: 17947.2891\n",
      "[449] loss: 17947.3594\n",
      "[450] loss: 17947.2227\n",
      "[451] loss: 17947.1543\n",
      "[452] loss: 17947.1348\n",
      "[453] loss: 17947.1582\n",
      "[454] loss: 17947.2070\n",
      "[455] loss: 17947.2793\n",
      "[456] loss: 17947.3418\n",
      "[457] loss: 17947.2734\n",
      "[458] loss: 17947.2285\n",
      "[459] loss: 17947.2207\n",
      "[460] loss: 17947.2441\n",
      "[461] loss: 17947.2773\n",
      "[462] loss: 17947.3047\n",
      "[463] loss: 17947.1777\n",
      "[464] loss: 17947.0938\n",
      "[465] loss: 17947.0664\n",
      "[466] loss: 17947.0859\n",
      "[467] loss: 17947.1328\n",
      "[468] loss: 17947.1953\n",
      "[469] loss: 17947.2812\n",
      "[470] loss: 17947.3262\n",
      "[471] loss: 17947.1309\n",
      "[472] loss: 17947.0000\n",
      "[473] loss: 17946.9395\n",
      "[474] loss: 17946.9492\n",
      "[475] loss: 17947.0078\n",
      "[476] loss: 17947.0781\n",
      "[477] loss: 17947.1719\n",
      "[478] loss: 17947.2578\n",
      "[479] loss: 17947.3359\n",
      "[480] loss: 17947.3281\n",
      "[481] loss: 17947.2305\n",
      "[482] loss: 17947.1758\n",
      "[483] loss: 17947.1641\n",
      "[484] loss: 17947.1875\n",
      "[485] loss: 17947.2207\n",
      "[486] loss: 17947.2500\n",
      "[487] loss: 17947.2402\n",
      "[488] loss: 17947.1797\n",
      "[489] loss: 17947.1445\n",
      "[490] loss: 17947.1484\n",
      "[491] loss: 17947.1836\n",
      "[492] loss: 17947.2422\n",
      "[493] loss: 17947.2695\n",
      "[494] loss: 17947.0723\n",
      "[495] loss: 17946.9258\n",
      "[496] loss: 17946.8457\n",
      "[497] loss: 17946.8379\n",
      "[498] loss: 17946.8828\n",
      "[499] loss: 17946.9590\n",
      "[500] loss: 17947.0527\n",
      "[501] loss: 17947.1562\n",
      "[502] loss: 17947.2461\n",
      "[503] loss: 17947.3164\n",
      "[504] loss: 17947.0684\n",
      "[505] loss: 17946.9062\n",
      "[506] loss: 17946.8203\n",
      "[507] loss: 17946.7969\n",
      "[508] loss: 17946.8203\n",
      "[509] loss: 17946.8672\n",
      "[510] loss: 17946.9297\n",
      "[511] loss: 17947.0078\n",
      "[512] loss: 17947.1094\n",
      "[513] loss: 17947.2090\n",
      "[514] loss: 17947.2949\n",
      "[515] loss: 17947.1562\n",
      "[516] loss: 17947.0527\n",
      "[517] loss: 17947.0039\n",
      "[518] loss: 17946.9902\n",
      "[519] loss: 17947.0039\n",
      "[520] loss: 17947.0352\n",
      "[521] loss: 17947.0762\n",
      "[522] loss: 17947.1367\n",
      "[523] loss: 17947.1914\n",
      "[524] loss: 17947.2109\n",
      "[525] loss: 17947.0430\n",
      "[526] loss: 17946.9336\n",
      "[527] loss: 17946.8730\n",
      "[528] loss: 17946.8691\n",
      "[529] loss: 17946.8945\n",
      "[530] loss: 17946.9355\n",
      "[531] loss: 17946.9922\n",
      "[532] loss: 17947.0703\n",
      "[533] loss: 17947.1465\n",
      "[534] loss: 17947.2246\n",
      "[535] loss: 17947.0156\n",
      "[536] loss: 17946.8711\n",
      "[537] loss: 17946.7852\n",
      "[538] loss: 17946.7500\n",
      "[539] loss: 17946.7598\n",
      "[540] loss: 17946.7969\n",
      "[541] loss: 17946.8555\n",
      "[542] loss: 17946.9336\n",
      "[543] loss: 17947.0352\n",
      "[544] loss: 17947.1309\n",
      "[545] loss: 17947.2188\n",
      "[546] loss: 17947.0664\n",
      "[547] loss: 17946.9551\n",
      "[548] loss: 17946.8984\n",
      "[549] loss: 17946.8750\n",
      "[550] loss: 17946.8828\n",
      "[551] loss: 17946.9102\n",
      "[552] loss: 17946.9531\n",
      "[553] loss: 17947.0098\n",
      "[554] loss: 17947.0840\n",
      "[555] loss: 17947.1445\n",
      "[556] loss: 17947.0156\n",
      "[557] loss: 17946.9375\n",
      "[558] loss: 17946.9023\n",
      "[559] loss: 17946.9023\n",
      "[560] loss: 17946.9238\n",
      "[561] loss: 17946.9570\n",
      "[562] loss: 17947.0000\n",
      "[563] loss: 17947.0664\n",
      "[564] loss: 17947.1094\n",
      "[565] loss: 17946.9727\n",
      "[566] loss: 17946.8828\n",
      "[567] loss: 17946.8320\n",
      "[568] loss: 17946.8262\n",
      "[569] loss: 17946.8516\n",
      "[570] loss: 17946.8906\n",
      "[571] loss: 17946.9414\n",
      "[572] loss: 17947.0156\n",
      "[573] loss: 17947.0801\n",
      "[574] loss: 17947.0938\n",
      "[575] loss: 17946.8750\n",
      "[576] loss: 17946.7148\n",
      "[577] loss: 17946.6270\n",
      "[578] loss: 17946.6035\n",
      "[579] loss: 17946.6270\n",
      "[580] loss: 17946.6719\n",
      "[581] loss: 17946.7266\n",
      "[582] loss: 17946.8008\n",
      "[583] loss: 17946.8945\n",
      "[584] loss: 17947.0000\n",
      "[585] loss: 17947.0918\n",
      "[586] loss: 17947.1367\n",
      "[587] loss: 17946.8301\n",
      "[588] loss: 17946.5938\n",
      "[589] loss: 17946.4414\n",
      "[590] loss: 17946.3672\n",
      "[591] loss: 17946.3672\n",
      "[592] loss: 17946.4082\n",
      "[593] loss: 17946.4785\n",
      "[594] loss: 17946.5586\n",
      "[595] loss: 17946.6445\n",
      "[596] loss: 17946.7539\n",
      "[597] loss: 17946.8906\n",
      "[598] loss: 17947.0195\n",
      "[599] loss: 17947.1289\n",
      "[600] loss: 17947.2070\n",
      "[601] loss: 17946.7949\n",
      "[602] loss: 17946.4824\n",
      "[603] loss: 17946.2559\n",
      "[604] loss: 17946.1250\n",
      "[605] loss: 17946.0820\n",
      "[606] loss: 17946.1035\n",
      "[607] loss: 17946.1602\n",
      "[608] loss: 17946.2480\n",
      "[609] loss: 17946.3594\n",
      "[610] loss: 17946.4844\n",
      "[611] loss: 17946.6426\n",
      "[612] loss: 17946.8223\n",
      "[613] loss: 17946.9883\n",
      "[614] loss: 17947.1133\n",
      "[615] loss: 17947.2012\n",
      "[616] loss: 17947.2227\n",
      "[617] loss: 17947.0234\n",
      "[618] loss: 17946.8535\n",
      "[619] loss: 17946.7578\n",
      "[620] loss: 17946.6973\n",
      "[621] loss: 17946.6699\n",
      "[622] loss: 17946.6699\n",
      "[623] loss: 17946.6914\n",
      "[624] loss: 17946.7266\n",
      "[625] loss: 17946.7773\n",
      "[626] loss: 17946.8398\n",
      "[627] loss: 17946.9336\n",
      "[628] loss: 17947.0254\n",
      "[629] loss: 17947.1016\n",
      "[630] loss: 17946.9141\n",
      "[631] loss: 17946.7773\n",
      "[632] loss: 17946.6875\n",
      "[633] loss: 17946.6367\n",
      "[634] loss: 17946.6270\n",
      "[635] loss: 17946.6426\n",
      "[636] loss: 17946.6797\n",
      "[637] loss: 17946.7266\n",
      "[638] loss: 17946.7773\n",
      "[639] loss: 17946.8457\n",
      "[640] loss: 17946.9277\n",
      "[641] loss: 17947.0039\n",
      "[642] loss: 17947.0508\n",
      "[643] loss: 17946.8398\n",
      "[644] loss: 17946.6953\n",
      "[645] loss: 17946.5918\n",
      "[646] loss: 17946.5352\n",
      "[647] loss: 17946.5254\n",
      "[648] loss: 17946.5430\n",
      "[649] loss: 17946.5820\n",
      "[650] loss: 17946.6328\n",
      "[651] loss: 17946.6992\n",
      "[652] loss: 17946.7773\n",
      "[653] loss: 17946.8711\n",
      "[654] loss: 17946.9570\n",
      "[655] loss: 17947.0312\n",
      "[656] loss: 17946.8242\n",
      "[657] loss: 17946.6758\n",
      "[658] loss: 17946.5762\n",
      "[659] loss: 17946.5156\n",
      "[660] loss: 17946.5039\n",
      "[661] loss: 17946.5234\n",
      "[662] loss: 17946.5684\n",
      "[663] loss: 17946.6211\n",
      "[664] loss: 17946.6836\n",
      "[665] loss: 17946.7559\n",
      "[666] loss: 17946.8418\n",
      "[667] loss: 17946.9258\n",
      "[668] loss: 17947.0078\n",
      "[669] loss: 17946.7988\n",
      "[670] loss: 17946.6523\n",
      "[671] loss: 17946.5469\n",
      "[672] loss: 17946.4902\n",
      "[673] loss: 17946.4707\n",
      "[674] loss: 17946.4844\n",
      "[675] loss: 17946.5176\n",
      "[676] loss: 17946.5664\n",
      "[677] loss: 17946.6309\n",
      "[678] loss: 17946.7070\n",
      "[679] loss: 17946.7969\n",
      "[680] loss: 17946.8828\n",
      "[681] loss: 17946.9590\n",
      "[682] loss: 17946.9570\n",
      "[683] loss: 17946.8867\n",
      "[684] loss: 17946.8340\n",
      "[685] loss: 17946.8066\n",
      "[686] loss: 17946.7969\n",
      "[687] loss: 17946.8008\n",
      "[688] loss: 17946.8164\n",
      "[689] loss: 17946.8262\n",
      "[690] loss: 17946.8223\n",
      "[691] loss: 17946.8203\n",
      "[692] loss: 17946.8184\n",
      "[693] loss: 17946.8066\n",
      "[694] loss: 17946.8125\n",
      "[695] loss: 17946.8320\n",
      "[696] loss: 17946.7871\n",
      "[697] loss: 17946.7773\n",
      "[698] loss: 17946.7969\n",
      "[699] loss: 17946.8105\n",
      "[700] loss: 17946.8066\n",
      "[701] loss: 17946.7988\n",
      "[702] loss: 17946.7891\n",
      "[703] loss: 17946.7891\n",
      "[704] loss: 17946.7910\n",
      "[705] loss: 17946.7695\n",
      "[706] loss: 17946.7715\n",
      "[707] loss: 17946.7930\n",
      "[708] loss: 17946.8105\n",
      "[709] loss: 17946.7500\n",
      "[710] loss: 17946.7383\n",
      "[711] loss: 17946.7578\n",
      "[712] loss: 17946.7832\n",
      "[713] loss: 17946.7871\n",
      "[714] loss: 17946.7188\n",
      "[715] loss: 17946.6777\n",
      "[716] loss: 17946.6641\n",
      "[717] loss: 17946.6777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[718] loss: 17946.7070\n",
      "[719] loss: 17946.7500\n",
      "[720] loss: 17946.7773\n",
      "[721] loss: 17946.7695\n",
      "[722] loss: 17946.7363\n",
      "[723] loss: 17946.7188\n",
      "[724] loss: 17946.7305\n",
      "[725] loss: 17946.7617\n",
      "[726] loss: 17946.7207\n",
      "[727] loss: 17946.7109\n",
      "[728] loss: 17946.7305\n",
      "[729] loss: 17946.7422\n",
      "[730] loss: 17946.7422\n",
      "[731] loss: 17946.7344\n",
      "[732] loss: 17946.7266\n",
      "[733] loss: 17946.7266\n",
      "[734] loss: 17946.7344\n",
      "[735] loss: 17946.6777\n",
      "[736] loss: 17946.6445\n",
      "[737] loss: 17946.6445\n",
      "[738] loss: 17946.6660\n",
      "[739] loss: 17946.7070\n",
      "[740] loss: 17946.7520\n",
      "[741] loss: 17946.7461\n",
      "[742] loss: 17946.7148\n",
      "[743] loss: 17946.7031\n",
      "[744] loss: 17946.7051\n",
      "[745] loss: 17946.7148\n",
      "[746] loss: 17946.7266\n",
      "[747] loss: 17946.6562\n",
      "[748] loss: 17946.6055\n",
      "[749] loss: 17946.5879\n",
      "[750] loss: 17946.5977\n",
      "[751] loss: 17946.6250\n",
      "[752] loss: 17946.6719\n",
      "[753] loss: 17946.7246\n",
      "[754] loss: 17946.7344\n",
      "[755] loss: 17946.7168\n",
      "[756] loss: 17946.6875\n",
      "[757] loss: 17946.6719\n",
      "[758] loss: 17946.6875\n",
      "[759] loss: 17946.7031\n",
      "[760] loss: 17946.6641\n",
      "[761] loss: 17946.6602\n",
      "[762] loss: 17946.6699\n",
      "[763] loss: 17946.6914\n",
      "[764] loss: 17946.7031\n",
      "[765] loss: 17946.6094\n",
      "[766] loss: 17946.5488\n",
      "[767] loss: 17946.5273\n",
      "[768] loss: 17946.5312\n",
      "[769] loss: 17946.5625\n",
      "[770] loss: 17946.6055\n",
      "[771] loss: 17946.6562\n",
      "[772] loss: 17946.7051\n",
      "[773] loss: 17946.7266\n",
      "[774] loss: 17946.6250\n",
      "[775] loss: 17946.5703\n",
      "[776] loss: 17946.5449\n",
      "[777] loss: 17946.5430\n",
      "[778] loss: 17946.5586\n",
      "[779] loss: 17946.5820\n",
      "[780] loss: 17946.6211\n",
      "[781] loss: 17946.6699\n",
      "[782] loss: 17946.7148\n",
      "[783] loss: 17946.5957\n",
      "[784] loss: 17946.5195\n",
      "[785] loss: 17946.4805\n",
      "[786] loss: 17946.4688\n",
      "[787] loss: 17946.4844\n",
      "[788] loss: 17946.5137\n",
      "[789] loss: 17946.5566\n",
      "[790] loss: 17946.6133\n",
      "[791] loss: 17946.6699\n",
      "[792] loss: 17946.7227\n",
      "[793] loss: 17946.6562\n",
      "[794] loss: 17946.6094\n",
      "[795] loss: 17946.5859\n",
      "[796] loss: 17946.5840\n",
      "[797] loss: 17946.5938\n",
      "[798] loss: 17946.6133\n",
      "[799] loss: 17946.6406\n",
      "[800] loss: 17946.6602\n",
      "[801] loss: 17946.6621\n",
      "[802] loss: 17946.6348\n",
      "[803] loss: 17946.6191\n",
      "[804] loss: 17946.6172\n",
      "[805] loss: 17946.6289\n",
      "[806] loss: 17946.6328\n",
      "[807] loss: 17946.6328\n",
      "[808] loss: 17946.6328\n",
      "[809] loss: 17946.6191\n",
      "[810] loss: 17946.6230\n",
      "[811] loss: 17946.6348\n",
      "[812] loss: 17946.5840\n",
      "[813] loss: 17946.5547\n",
      "[814] loss: 17946.5469\n",
      "[815] loss: 17946.5586\n",
      "[816] loss: 17946.5859\n",
      "[817] loss: 17946.6250\n",
      "[818] loss: 17946.6406\n",
      "[819] loss: 17946.6309\n",
      "[820] loss: 17946.6191\n",
      "[821] loss: 17946.5977\n",
      "[822] loss: 17946.6055\n",
      "[823] loss: 17946.6113\n",
      "[824] loss: 17946.5859\n",
      "[825] loss: 17946.5898\n",
      "[826] loss: 17946.5996\n",
      "[827] loss: 17946.6133\n",
      "[828] loss: 17946.6055\n",
      "[829] loss: 17946.5957\n",
      "[830] loss: 17946.5918\n",
      "[831] loss: 17946.5957\n",
      "[832] loss: 17946.6055\n",
      "[833] loss: 17946.5312\n",
      "[834] loss: 17946.4805\n",
      "[835] loss: 17946.4609\n",
      "[836] loss: 17946.4688\n",
      "[837] loss: 17946.4961\n",
      "[838] loss: 17946.5293\n",
      "[839] loss: 17946.5781\n",
      "[840] loss: 17946.6289\n",
      "[841] loss: 17946.6484\n",
      "[842] loss: 17946.4473\n",
      "[843] loss: 17946.3008\n",
      "[844] loss: 17946.2188\n",
      "[845] loss: 17946.1836\n",
      "[846] loss: 17946.1816\n",
      "[847] loss: 17946.2070\n",
      "[848] loss: 17946.2520\n",
      "[849] loss: 17946.3203\n",
      "[850] loss: 17946.3848\n",
      "[851] loss: 17946.4707\n",
      "[852] loss: 17946.5723\n",
      "[853] loss: 17946.6523\n",
      "[854] loss: 17946.7207\n",
      "[855] loss: 17946.5898\n",
      "[856] loss: 17946.4863\n",
      "[857] loss: 17946.4375\n",
      "[858] loss: 17946.4160\n",
      "[859] loss: 17946.4160\n",
      "[860] loss: 17946.4297\n",
      "[861] loss: 17946.4434\n",
      "[862] loss: 17946.4766\n",
      "[863] loss: 17946.5156\n",
      "[864] loss: 17946.5742\n",
      "[865] loss: 17946.6348\n",
      "[866] loss: 17946.7070\n",
      "[867] loss: 17946.2930\n",
      "[868] loss: 17945.9609\n",
      "[869] loss: 17945.7266\n",
      "[870] loss: 17945.5898\n",
      "[871] loss: 17945.5547\n",
      "[872] loss: 17945.5469\n",
      "[873] loss: 17945.6250\n",
      "[874] loss: 17945.7051\n",
      "[875] loss: 17945.8125\n",
      "[876] loss: 17945.9531\n",
      "[877] loss: 17946.1328\n",
      "[878] loss: 17946.3086\n",
      "[879] loss: 17946.4570\n",
      "[880] loss: 17946.5645\n",
      "[881] loss: 17946.6445\n",
      "[882] loss: 17946.7188\n",
      "[883] loss: 17946.7500\n",
      "[884] loss: 17946.7793\n",
      "[885] loss: 17946.3750\n",
      "[886] loss: 17946.0625\n",
      "[887] loss: 17945.8262\n",
      "[888] loss: 17945.6953\n",
      "[889] loss: 17945.6562\n",
      "[890] loss: 17945.6621\n",
      "[891] loss: 17945.7090\n",
      "[892] loss: 17945.7773\n",
      "[893] loss: 17945.8906\n",
      "[894] loss: 17946.0176\n",
      "[895] loss: 17946.1562\n",
      "[896] loss: 17946.2930\n",
      "[897] loss: 17946.4062\n",
      "[898] loss: 17946.5039\n",
      "[899] loss: 17946.5879\n",
      "[900] loss: 17946.6484\n",
      "[901] loss: 17946.6797\n",
      "[902] loss: 17946.6953\n",
      "[903] loss: 17946.6953\n",
      "[904] loss: 17946.6895\n",
      "[905] loss: 17946.6582\n",
      "[906] loss: 17946.6250\n",
      "[907] loss: 17946.6094\n",
      "[908] loss: 17946.6289\n",
      "[909] loss: 17946.6191\n",
      "[910] loss: 17946.5859\n",
      "[911] loss: 17946.5625\n",
      "[912] loss: 17946.5547\n",
      "[913] loss: 17946.5547\n",
      "[914] loss: 17946.5605\n",
      "[915] loss: 17946.5762\n",
      "[916] loss: 17946.5801\n",
      "[917] loss: 17946.5762\n",
      "[918] loss: 17946.5703\n",
      "[919] loss: 17946.5703\n",
      "[920] loss: 17946.5762\n",
      "[921] loss: 17946.5703\n",
      "[922] loss: 17946.5625\n",
      "[923] loss: 17946.5605\n",
      "[924] loss: 17946.5645\n",
      "[925] loss: 17946.5703\n",
      "[926] loss: 17946.5371\n",
      "[927] loss: 17946.5195\n",
      "[928] loss: 17946.5137\n",
      "[929] loss: 17946.5176\n",
      "[930] loss: 17946.5312\n",
      "[931] loss: 17946.5586\n",
      "[932] loss: 17946.5137\n",
      "[933] loss: 17946.4883\n",
      "[934] loss: 17946.4844\n",
      "[935] loss: 17946.4941\n",
      "[936] loss: 17946.5137\n",
      "[937] loss: 17946.5430\n",
      "[938] loss: 17946.5547\n",
      "[939] loss: 17946.5508\n",
      "[940] loss: 17946.5371\n",
      "[941] loss: 17946.5254\n",
      "[942] loss: 17946.5332\n",
      "[943] loss: 17946.5430\n",
      "[944] loss: 17946.4688\n",
      "[945] loss: 17946.4082\n",
      "[946] loss: 17946.3809\n",
      "[947] loss: 17946.3770\n",
      "[948] loss: 17946.3867\n",
      "[949] loss: 17946.4062\n",
      "[950] loss: 17946.4375\n",
      "[951] loss: 17946.4805\n",
      "[952] loss: 17946.5312\n",
      "[953] loss: 17946.5820\n",
      "[954] loss: 17946.4766\n",
      "[955] loss: 17946.4102\n",
      "[956] loss: 17946.3711\n",
      "[957] loss: 17946.3555\n",
      "[958] loss: 17946.3555\n",
      "[959] loss: 17946.3652\n",
      "[960] loss: 17946.3906\n",
      "[961] loss: 17946.4199\n",
      "[962] loss: 17946.4668\n",
      "[963] loss: 17946.5195\n",
      "[964] loss: 17946.5664\n",
      "[965] loss: 17946.5625\n",
      "[966] loss: 17946.5098\n",
      "[967] loss: 17946.4824\n",
      "[968] loss: 17946.4688\n",
      "[969] loss: 17946.4688\n",
      "[970] loss: 17946.4746\n",
      "[971] loss: 17946.4883\n",
      "[972] loss: 17946.5098\n",
      "[973] loss: 17946.5352\n",
      "[974] loss: 17946.4570\n",
      "[975] loss: 17946.4023\n",
      "[976] loss: 17946.3809\n",
      "[977] loss: 17946.3789\n",
      "[978] loss: 17946.3867\n",
      "[979] loss: 17946.4004\n",
      "[980] loss: 17946.4238\n",
      "[981] loss: 17946.4629\n",
      "[982] loss: 17946.5039\n",
      "[983] loss: 17946.5508\n",
      "[984] loss: 17946.4121\n",
      "[985] loss: 17946.3125\n",
      "[986] loss: 17946.2480\n",
      "[987] loss: 17946.2188\n",
      "[988] loss: 17946.2109\n",
      "[989] loss: 17946.2266\n",
      "[990] loss: 17946.2500\n",
      "[991] loss: 17946.2852\n",
      "[992] loss: 17946.3359\n",
      "[993] loss: 17946.4062\n",
      "[994] loss: 17946.4727\n",
      "[995] loss: 17946.5352\n",
      "[996] loss: 17946.5801\n",
      "[997] loss: 17946.5039\n",
      "[998] loss: 17946.4492\n",
      "[999] loss: 17946.4180\n",
      "[1000] loss: 17946.4062\n",
      "Elapsed time: 31523.07 seconds\n",
      "Top-1% accuracy: 0.36\n",
      "Top-5% accuracy: 0.18\n",
      "Top-10% accuracy: 0.18\n",
      "Kendall tau distance: 0.00\n",
      "Running time: 31523.07\n"
     ]
    }
   ],
   "source": [
    "com_top1,com_top5,com_top10,com_ken,com_elapsed_time = train_and_result_for_once(filenames[1])\n",
    "print(\"Top-1% accuracy: {:.2f}\".format(com_top1))\n",
    "print(\"Top-5% accuracy: {:.2f}\".format(com_top5))\n",
    "print(\"Top-10% accuracy: {:.2f}\".format(com_top10))\n",
    "print(\"Kendall tau distance: {:.2f}\".format(com_ken))\n",
    "print(\"Running time: {:.2f}\".format(com_elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047c1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_result_for_com(filename):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    k = [1,5,10]\n",
    "    \n",
    "    print(\"Working on {}\".format(filename))\n",
    "    # Prepare the data\n",
    "    G = gList[filename]['graph']\n",
    "    y = torch.tensor([list(gList[filename]['score'].values())])\n",
    "    y = torch.transpose(y,0,1)\n",
    "\n",
    "    # Define the models\n",
    "    input_size = 3\n",
    "    hidden_size = 3\n",
    "    output_size = 1\n",
    "    num_layers = 1\n",
    "    encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "    decoder = DrBCDecoder(hidden_size,hidden_size,output_size)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    model = EncoderDecoder(encoder,decoder)\n",
    "    model.to(device)\n",
    "    \n",
    "    n = G.number_of_nodes()\n",
    "    num_episodes = 5\n",
    "    lr = 0.001\n",
    "    sample_qty = int(0.5*n) # Reduce to 1.5|N|\n",
    "\n",
    "    # Define the loss and optimizer\n",
    "    criterion = nn.BCELoss(reduction = 'mean') # Sum may leak\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction = 'sum') # Because only pred will go through sigmoid, but ground truth won't\n",
    "    #optimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get the inputs\n",
    "    inputs = gen_nodes_feature(G)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Start training on {}\".format(device))\n",
    "    pairs = sampling(0,n-1,sample_qty)\n",
    "    for episode in range(num_episodes):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # model\n",
    "        outputs = model(inputs)\n",
    "        print(\"finish output from model\")\n",
    "        pred,gt = bc_pairs(pairs,outputs,y)\n",
    "        loss = criterion(pred,gt)\n",
    "        if ~loss.requires_grad:\n",
    "            loss.requires_grad_()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if best_loss>loss:\n",
    "            best_loss = loss\n",
    "            best_out = outputs\n",
    "            #best_model_weights = model.state_dict()\n",
    "            \n",
    "        # Print statistics\n",
    "        #if episode%5 == 4:\n",
    "        print('[%d] loss: %.4f' %(episode + 1, loss.item()))\n",
    "            \n",
    "    #torch.save(best_model_weights, 'best_model.pth')\n",
    "    top1 = topN(1,best_out,y)\n",
    "    top5 = topN(5,best_out,y)\n",
    "    top10 = topN(10,best_out,y)\n",
    "    ken = kendall(best_out,y)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "    return top1,top5,top10,ken,elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on com-youtube.txt\n",
      "Start training on cuda\n"
     ]
    }
   ],
   "source": [
    "com_top1,com_top5,com_top10,com_ken,com_elapsed_time = train_and_result(filenames[-1])\n",
    "print(\"Top-1% accuracy: {:.2f}\".format(com_top1))\n",
    "print(\"Top-5% accuracy: {:.2f}\".format(com_top5))\n",
    "print(\"Top-10% accuracy: {:.2f}\".format(com_top10))\n",
    "print(\"Kendall tau distance: {:.2f}\".format(com_ken))\n",
    "print(\"Running time: {:.2f}\".format(com_elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e76ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
