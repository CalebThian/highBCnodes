{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbf6a13",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb22d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "698f5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        print(f\"Running {func.__name__} ...\", end='\\r')\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} Done in {end - start:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206514a0",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing  \n",
    "Data Structure:\n",
    "1. **gList** <Dict>: containing total 31 graphs, which 30 from Synthetic and 1 from youtube,using filename as key  \n",
    "2. element of gList <Dict>: 'graph':nx.Graph();'score': <Dict> with 'node' and 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b0a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt has 5000 nodes, 19982 edges\n",
      "1.txt has 5000 nodes, 19981 edges\n",
      "10.txt has 5000 nodes, 19980 edges\n",
      "11.txt has 5000 nodes, 19983 edges\n",
      "12.txt has 5000 nodes, 19983 edges\n",
      "13.txt has 5000 nodes, 19984 edges\n",
      "14.txt has 5000 nodes, 19982 edges\n",
      "15.txt has 5000 nodes, 19984 edges\n",
      "16.txt has 5000 nodes, 19982 edges\n",
      "17.txt has 5000 nodes, 19981 edges\n",
      "18.txt has 5000 nodes, 19984 edges\n",
      "19.txt has 5000 nodes, 19981 edges\n",
      "2.txt has 5000 nodes, 19980 edges\n",
      "20.txt has 5000 nodes, 19983 edges\n",
      "21.txt has 5000 nodes, 19982 edges\n",
      "22.txt has 5000 nodes, 19982 edges\n",
      "23.txt has 5000 nodes, 19981 edges\n",
      "24.txt has 5000 nodes, 19984 edges\n",
      "25.txt has 5000 nodes, 19982 edges\n",
      "26.txt has 5000 nodes, 19984 edges\n",
      "27.txt has 5000 nodes, 19983 edges\n",
      "28.txt has 5000 nodes, 19982 edges\n",
      "29.txt has 5000 nodes, 19983 edges\n",
      "3.txt has 5000 nodes, 19982 edges\n",
      "4.txt has 5000 nodes, 19984 edges\n",
      "5.txt has 5000 nodes, 19981 edges\n",
      "6.txt has 5000 nodes, 19984 edges\n",
      "7.txt has 5000 nodes, 19983 edges\n",
      "8.txt has 5000 nodes, 19983 edges\n",
      "9.txt has 5000 nodes, 19984 edges\n",
      "com-youtube.txt has 0 nodes, 0 edges\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "dpath = \".\\\\data\\\\\"\n",
    "gList = dict()\n",
    "\n",
    "for root, dirs, files in os.walk(dpath):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if 'score' not in file:\n",
    "            # Process nodes and edges\n",
    "            gList[file] = dict()\n",
    "            gList[file]['graph']=nx.Graph()\n",
    "            with open(file_path,'r') as f:\n",
    "                content = f.readlines()\n",
    "                edges = []\n",
    "                for line in content:\n",
    "                    if 'com' not in file:\n",
    "                        nodes = line[:-1].split('\\t')\n",
    "                    else:\n",
    "                        continue # after finish all code run code with com\n",
    "                        nodes = line[:-1].split(\" \")\n",
    "                    # Create edge tuple and append\n",
    "                    edges.append((int(nodes[0]),int(nodes[1])))\n",
    "                gList[file]['graph'].add_edges_from(edges)\n",
    "                print(\"{} has {} nodes, {} edges\".format(file,gList[file]['graph'].number_of_nodes(),gList[file]['graph'].number_of_edges()))\n",
    "            \n",
    "            # Process scores\n",
    "            scorefile = file.replace(\".txt\",\"_score.txt\")\n",
    "            gList[file]['score'] = dict()\n",
    "            score_file_path = os.path.join(root,scorefile) \n",
    "            with open(score_file_path,'r') as f:\n",
    "                content = f.readlines()\n",
    "                for line in content:\n",
    "                    if 'com' not in file:\n",
    "                        node_score = line[:-1].split('\\t')\n",
    "                    else:\n",
    "                        continue # after finish all code run code with com\n",
    "                        node_score = line[:-1].split(\" \")\n",
    "                    gList[file]['score'][int(node_score[0])] = float(node_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd0956",
   "metadata": {},
   "source": [
    "# 3. DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e286c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.4175e-02, 5.3971e-02, 4.4344e-02,  ..., 1.9542e-05, 6.0578e-05,\n",
       "        1.0908e-04])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = gList['0.txt']['graph']\n",
    "y = torch.tensor(list(gList['0.txt']['score'].values()))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72b7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare nodes initial feature X [dv,1,1]\n",
    "def gen_nodes_feature(G):\n",
    "    deg = np.array(list(dict(sorted(dict(G.degree()).items())).values()))\n",
    "    X = np.ones((3,len(deg)))\n",
    "    X[0,:]=deg\n",
    "    norms = np.linalg.norm(X,axis = 1,keepdims=True)\n",
    "    X = torch.FloatTensor(X.T)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784a1ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1)\n",
      "tensor([[1.0000, 0.0042, 0.0042],\n",
      "        [1.0000, 0.0056, 0.0056],\n",
      "        [1.0000, 0.0067, 0.0067],\n",
      "        ...,\n",
      "        [0.9428, 0.2357, 0.2357],\n",
      "        [0.9428, 0.2357, 0.2357],\n",
      "        [0.9428, 0.2357, 0.2357]])\n"
     ]
    }
   ],
   "source": [
    "X=gen_nodes_feature(g)\n",
    "norms = np.linalg.norm(X,axis = 1,keepdims=True)\n",
    "print(norms.shape)\n",
    "X_norm = X/norms\n",
    "print(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d99f2d",
   "metadata": {},
   "source": [
    "## 3a. DrBC encoder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f09fe1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the model\\ninput_size = 3\\nhidden_size = 32\\nnum_layers = 5\\nencoder = DrBCEncoder(input_size, hidden_size, num_layers,g)\\nX = gen_nodes_feature(g)\\nout = encoder(X)\\nprint(out.shape)\\nprint(out)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DrBCEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,G):\n",
    "        super(DrBCEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.gru_cell = nn.GRUCell(hidden_size, hidden_size,bias = False)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.G = G\n",
    "        self.deg = dict(self.G.degree())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        output = [x]\n",
    "        for i in range(self.num_layers-1):\n",
    "            hn = self.calHn(x)\n",
    "            x = self.gru_cell(x,hn)\n",
    "            x = self.norm2(x)\n",
    "            output.append(x)\n",
    "        output, _ = torch.max(torch.stack(output), dim=0)\n",
    "        return output\n",
    "\n",
    "    def calHn(self,x):\n",
    "        hn = torch.zeros(x.shape)\n",
    "        for node in self.G.nodes():\n",
    "            degv = self.deg[node]\n",
    "            for neigh in list(self.G.adj[node]):\n",
    "                denominator = 1/(math.sqrt(degv+1)*math.sqrt(self.deg[neigh]+1))\n",
    "                hn[node,:] += (denominator*x[neigh])\n",
    "        return hn\n",
    "    \n",
    "'''\n",
    "# Define the model\n",
    "input_size = 3\n",
    "hidden_size = 32\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,g)\n",
    "X = gen_nodes_feature(g)\n",
    "out = encoder(X)\n",
    "print(out.shape)\n",
    "print(out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c729bc",
   "metadata": {},
   "source": [
    "## 3b. Decoder: 2-layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8040f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrBCDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DrBCDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the layers of the decoder\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.norm2 = nn.BatchNorm1d(output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the layers of the decoder\n",
    "        x = self.layer1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc87fb5",
   "metadata": {},
   "source": [
    "## 3c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1881ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(minimum,maximum,qty):\n",
    "    pairs = []\n",
    "    for i in range(qty):\n",
    "        a = random.randint(minimum,maximum)\n",
    "        b = random.randint(minimum,maximum)\n",
    "        pairs.append((a,b))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4afd51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bc_pairs(pairs,pred,gt):\n",
    "    pred_dif = []\n",
    "    gt_dif = []\n",
    "    g = nn.Sigmoid()\n",
    "    for pair in pairs:\n",
    "        pred_dif.append(g(pred[pair[0]]-pred[pair[1]]))\n",
    "        gt_dif.append(g(gt[pair[0]]-gt[pair[1]]))\n",
    "    return torch.tensor(pred_dif),torch.tensor(gt_dif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c178029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gList['0.txt']['graph']\n",
    "y = torch.tensor([list(gList['0.txt']['score'].values())])\n",
    "y = torch.transpose(y,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48e04baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 22054.4902\n",
      "[2] loss: 22153.8184\n",
      "[3] loss: 22127.1387\n",
      "[4] loss: 22099.7559\n",
      "[5] loss: 22178.6367\n",
      "[6] loss: 22075.3633\n",
      "[7] loss: 22136.5566\n",
      "[8] loss: 22211.8926\n",
      "[9] loss: 22174.9141\n",
      "[10] loss: 22220.7363\n",
      "[11] loss: 22123.0059\n",
      "[12] loss: 22151.0078\n",
      "[13] loss: 22178.3594\n",
      "[14] loss: 22041.4180\n",
      "[15] loss: 22231.3164\n",
      "[16] loss: 22082.9863\n",
      "[17] loss: 22159.6211\n",
      "[18] loss: 22125.8262\n",
      "[19] loss: 22155.9180\n",
      "[20] loss: 22121.5527\n"
     ]
    }
   ],
   "source": [
    "# Define the models\n",
    "input_size = 3\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "decoder = DrBCDecoder(hidden_size,hidden_size,output_size)\n",
    "\n",
    "n = G.number_of_nodes()\n",
    "num_episodes = 20\n",
    "lr = 0.001\n",
    "sample_qty = 5*n\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.BCELoss(reduction = 'sum')\n",
    "optimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Get the inputs\n",
    "inputs = gen_nodes_feature(G)\n",
    "\n",
    "# Train the model\n",
    "for episode in range(num_episodes):\n",
    "    # model\n",
    "    outputs = encoder(inputs)\n",
    "    outputs = decoder(outputs)\n",
    "    \n",
    "    pairs = sampling(0,n-1,sample_qty)\n",
    "    pred,gt = bc_pairs(pairs,outputs,y)\n",
    "    loss = criterion(pred,gt)\n",
    "\n",
    "    if ~loss.requires_grad:\n",
    "        loss.requires_grad_()\n",
    "        \n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print statistics\n",
    "    print('[%d] loss: %.4f' %(episode + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd1a40",
   "metadata": {},
   "source": [
    "# 4. Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97af471",
   "metadata": {},
   "source": [
    "## 4a. Top-N% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "49e7b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(n,pred,gt):\n",
    "    k = math.ceil(pred.size()[0]*n/100)\n",
    "    _,pred_top = torch.topk(pred.view(-1),k=k)\n",
    "    _,gt_top = torch.topk(gt.view(-1),k=k)\n",
    "    intersect = torch.unique(torch.cat((pred_top,gt_top),0))\n",
    "    print((2*k-len(intersect))/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebeb3605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n"
     ]
    }
   ],
   "source": [
    "topN(1,outputs,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7e7e3",
   "metadata": {},
   "source": [
    "## 4b. Kendall tau distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "873d442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall(pred,gt):\n",
    "    pred_ind = torch.argsort(pred)\n",
    "    gt_ind = torch.argsort(gt)\n",
    "    con = 0 # number of concordant pairs\n",
    "    dcor = 0 # number of discordant pairs\n",
    "    n = len(pred_ind)\n",
    "    for i in range(n):\n",
    "        if pred_ind[i] == gt_ind[i]:\n",
    "            con += 1\n",
    "        else:\n",
    "            dcor += 1\n",
    "    return 2*(con-dcor)/(n*(n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f40e1239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00040008001600320064"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kendall(outputs,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b7409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
