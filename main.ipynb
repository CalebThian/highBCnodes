{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbf6a13",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb22d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206514a0",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing  \n",
    "Data Structure:\n",
    "1. **gList** <Dict>: containing total 31 graphs, which 30 from Synthetic and 1 from youtube,using filename as key  \n",
    "2. element of gList <Dict>: 'graph':nx.Graph();'score': <Dict> with 'node' and 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b0a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt has 5000 nodes, 19982 edges\n",
      "1.txt has 5000 nodes, 19981 edges\n",
      "10.txt has 5000 nodes, 19980 edges\n",
      "11.txt has 5000 nodes, 19983 edges\n",
      "12.txt has 5000 nodes, 19983 edges\n",
      "13.txt has 5000 nodes, 19984 edges\n",
      "14.txt has 5000 nodes, 19982 edges\n",
      "15.txt has 5000 nodes, 19984 edges\n",
      "16.txt has 5000 nodes, 19982 edges\n",
      "17.txt has 5000 nodes, 19981 edges\n",
      "18.txt has 5000 nodes, 19984 edges\n",
      "19.txt has 5000 nodes, 19981 edges\n",
      "2.txt has 5000 nodes, 19980 edges\n",
      "20.txt has 5000 nodes, 19983 edges\n",
      "21.txt has 5000 nodes, 19982 edges\n",
      "22.txt has 5000 nodes, 19982 edges\n",
      "23.txt has 5000 nodes, 19981 edges\n",
      "24.txt has 5000 nodes, 19984 edges\n",
      "25.txt has 5000 nodes, 19982 edges\n",
      "26.txt has 5000 nodes, 19984 edges\n",
      "27.txt has 5000 nodes, 19983 edges\n",
      "28.txt has 5000 nodes, 19982 edges\n",
      "29.txt has 5000 nodes, 19983 edges\n",
      "3.txt has 5000 nodes, 19982 edges\n",
      "4.txt has 5000 nodes, 19984 edges\n",
      "5.txt has 5000 nodes, 19981 edges\n",
      "6.txt has 5000 nodes, 19984 edges\n",
      "7.txt has 5000 nodes, 19983 edges\n",
      "8.txt has 5000 nodes, 19983 edges\n",
      "9.txt has 5000 nodes, 19984 edges\n",
      "com-youtube.txt has 0 nodes, 0 edges\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "dpath = \".\\\\data\\\\\"\n",
    "gList = dict()\n",
    "\n",
    "for root, dirs, files in os.walk(dpath):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if 'score' not in file:\n",
    "            # Process nodes and edges\n",
    "            gList[file] = dict()\n",
    "            gList[file]['graph']=nx.Graph()\n",
    "            with open(file_path,'r') as f:\n",
    "                content = f.readlines()\n",
    "                edges = []\n",
    "                for line in content:\n",
    "                    if 'com' not in file:\n",
    "                        nodes = line[:-1].split('\\t')\n",
    "                    else:\n",
    "                        continue # after finish all code run code with com\n",
    "                        nodes = line[:-1].split(\" \")\n",
    "                    # Create edge tuple and append\n",
    "                    edges.append((int(nodes[0]),int(nodes[1])))\n",
    "                gList[file]['graph'].add_edges_from(edges)\n",
    "                print(\"{} has {} nodes, {} edges\".format(file,gList[file]['graph'].number_of_nodes(),gList[file]['graph'].number_of_edges()))\n",
    "            \n",
    "            # Process scores\n",
    "            scorefile = file.replace(\".txt\",\"_score.txt\")\n",
    "            gList[file]['score'] = dict()\n",
    "            score_file_path = os.path.join(root,scorefile) \n",
    "            with open(score_file_path,'r') as f:\n",
    "                content = f.readlines()\n",
    "                for line in content:\n",
    "                    if 'com' not in file:\n",
    "                        node_score = line[:-1].split('\\t')\n",
    "                    else:\n",
    "                        continue # after finish all code run code with com\n",
    "                        node_score = line[:-1].split(\" \")\n",
    "                    gList[file]['score'][int(node_score[0])] = float(node_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd0956",
   "metadata": {},
   "source": [
    "# 3. DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e286c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gList['0.txt']['graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72b7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare nodes initial feature X [dv,1,1]\n",
    "def gen_nodes_feature(G):\n",
    "    deg = np.array(list(dict(sorted(dict(g.degree()).items())).values()))\n",
    "    X = np.ones((3,len(deg)))\n",
    "    X[0,:]=deg\n",
    "    return X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784a1ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1)\n",
      "[[0.99998249 0.00418403 0.00418403]\n",
      " [0.99996844 0.0056178  0.0056178 ]\n",
      " [0.99995496 0.00671111 0.00671111]\n",
      " ...\n",
      " [0.94280904 0.23570226 0.23570226]\n",
      " [0.94280904 0.23570226 0.23570226]\n",
      " [0.94280904 0.23570226 0.23570226]]\n"
     ]
    }
   ],
   "source": [
    "X=gen_nodes_feature(g)\n",
    "norms = np.linalg.norm(X,axis = 1,keepdims=True)\n",
    "print(norms.shape)\n",
    "X_norm = X/norms\n",
    "print(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d99f2d",
   "metadata": {},
   "source": [
    "## 3a. DrBC encoder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75cd4588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 32])\n",
      "tensor([[-0.5139,  2.0427,  2.5759,  ...,  2.6434,  2.5304,  3.3350],\n",
      "        [-2.3558,  0.4919,  3.1532,  ...,  1.9962,  2.4635,  3.6587],\n",
      "        [-0.3457,  0.6449,  2.0438,  ...,  1.4955,  2.5203,  2.9065],\n",
      "        ...,\n",
      "        [ 2.0724,  1.3120, -0.3375,  ...,  0.9927,  0.4154,  0.0000],\n",
      "        [ 1.0374,  0.9425,  0.8812,  ...,  0.9927,  1.0440,  0.6344],\n",
      "        [ 1.0374,  2.0443,  1.1548,  ...,  1.5469,  1.4470,  0.0000]],\n",
      "       grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class DrBCEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,G):\n",
    "        super(DrBCEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.gru_cell = nn.GRUCell(hidden_size, hidden_size,bias = False)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.G = G\n",
    "        self.deg = dict(self.G.degree())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        output = [x]\n",
    "        for i in range(self.num_layers-1):\n",
    "            hn = self.calHn(x)\n",
    "            x = self.gru_cell(x,hn)\n",
    "            x = self.norm2(x)\n",
    "            output.append(x)\n",
    "        output, _ = torch.max(torch.stack(output), dim=0)\n",
    "        return output\n",
    "\n",
    "    def calHn(self,x):\n",
    "        hn = torch.zeros(x.shape)\n",
    "        for node in self.G.nodes():\n",
    "            degv = self.deg[node]\n",
    "            for neigh in list(self.G.adj[node]):\n",
    "                denominator = 1/(math.sqrt(degv+1)*math.sqrt(self.deg[neigh]+1))\n",
    "                hn[node,:] += (denominator*x[neigh])\n",
    "        return hn\n",
    "    \n",
    "# Define the model\n",
    "input_size = 3\n",
    "hidden_size = 32\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,g)\n",
    "out = encoder(torch.FloatTensor(X_norm))\n",
    "print(out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRU cell\n",
    "def GRU(hv,hn):\n",
    "    hv_t = torch.from_numpy(hv)\n",
    "    hn_t = torch.from_numpy(hn)\n",
    "    u = torch.sigmoid(hv_t+hn_t)\n",
    "    r = torch.sigmoid(hv_t+hn_t)\n",
    "    f = torch.tanh(torch.mul(hv_t,r)+hn_t)\n",
    "    return torch.mul(u,f)+torch.mul((1-u),hv_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db789f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(G,L = 5):\n",
    "    adj_mat = nx.to_numpy_matrix(g)\n",
    "    deg = dict(G.degree())\n",
    "    X = gen_nodes_feature(G)\n",
    "    H = []\n",
    "    norms1 = np.linalg.norm(X,axis = 1,keepdims=True)\n",
    "    H.append(X/norms1)\n",
    "    for l in range(1,L):\n",
    "        H.append(H[-1])\n",
    "        Hn = []\n",
    "        for node in G.nodes():\n",
    "            hn = 0\n",
    "            degv = deg[node]\n",
    "            for neigh in list(G.adj[node]):\n",
    "                den = math.sqrt(degv+1)*math.sqrt(deg[neigh]+1)\n",
    "                hn += H[l-1][neigh]/np.full(3,den)\n",
    "                H[l][node] = GRU(H[l-1][node],hn)\n",
    "        H[l] = H[l]/np.linalg.norm(H[l],axis = 1,keepdims=True)\n",
    "    return np.max(H,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85887f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_size = 3\n",
    "hidden_size = 64\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = encoder(torch.FloatTensor(X_norm))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df0dfafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]],\n",
      "\n",
      "        [[9, 8, 7],\n",
      "         [6, 5, 4],\n",
      "         [3, 2, 1]],\n",
      "\n",
      "        [[2, 4, 6],\n",
      "         [8, 7, 9],\n",
      "         [1, 3, 5]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[9, 8, 7],\n",
       "        [8, 7, 9],\n",
       "        [7, 8, 9]]),\n",
       "indices=tensor([[1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [0, 0, 0]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of matrices\n",
    "A = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "B = torch.tensor([[9, 8, 7], [6, 5, 4], [3, 2, 1]])\n",
    "C = torch.tensor([[2, 4, 6], [8, 7, 9], [1, 3, 5]])\n",
    "\n",
    "mat = torch.stack([A,B,C])\n",
    "\n",
    "# find maximum rows of each matrix\n",
    "print(mat)\n",
    "\n",
    "# concatenate maximum rows to form output matrix\n",
    "torch.max(mat, dim=0)\n",
    "\n",
    "\n",
    "# print output matrix\n",
    "#print(output_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7770b23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 0.5302, -0.3618,  0.3227, -0.1702],\n",
      "        [ 0.4083, -0.3624,  0.1434, -0.2685],\n",
      "        [-0.1534, -1.2297, -0.8141,  0.9433]]), tensor([[-0.3487, -0.9476, -0.5586,  0.6799],\n",
      "        [-1.2015, -1.3125, -0.9144, -1.3478],\n",
      "        [ 0.8463,  1.2838, -0.6756,  0.5241]]), tensor([[-0.0757,  0.5446, -0.0743,  0.3097],\n",
      "        [-0.3237,  0.0104,  1.4562,  0.3755],\n",
      "        [-0.7474, -1.5096,  1.4763,  0.5483]])]\n"
     ]
    }
   ],
   "source": [
    "for matrix i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae4c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # Get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = encoder(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bbfd4",
   "metadata": {},
   "source": [
    "## 3b. Decoder: 2-layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cac712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
