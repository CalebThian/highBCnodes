{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbf6a13",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb22d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206514a0",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing  \n",
    "Data Structure:\n",
    "1. **gList** <Dict>: containing total 31 graphs, which 30 from Synthetic and 1 from youtube,using filename as key  \n",
    "2. element of gList <Dict>: 'graph':nx.Graph();'score': <Dict> with 'node' and 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b0a52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.txt has 5000 nodes, 19982 edges\n",
      "1.txt has 5000 nodes, 19981 edges\n",
      "10.txt has 5000 nodes, 19980 edges\n",
      "11.txt has 5000 nodes, 19983 edges\n",
      "12.txt has 5000 nodes, 19983 edges\n",
      "13.txt has 5000 nodes, 19984 edges\n",
      "14.txt has 5000 nodes, 19982 edges\n",
      "15.txt has 5000 nodes, 19984 edges\n",
      "16.txt has 5000 nodes, 19982 edges\n",
      "17.txt has 5000 nodes, 19981 edges\n",
      "18.txt has 5000 nodes, 19984 edges\n",
      "19.txt has 5000 nodes, 19981 edges\n",
      "2.txt has 5000 nodes, 19980 edges\n",
      "20.txt has 5000 nodes, 19983 edges\n",
      "21.txt has 5000 nodes, 19982 edges\n",
      "22.txt has 5000 nodes, 19982 edges\n",
      "23.txt has 5000 nodes, 19981 edges\n",
      "24.txt has 5000 nodes, 19984 edges\n",
      "25.txt has 5000 nodes, 19982 edges\n",
      "26.txt has 5000 nodes, 19984 edges\n",
      "27.txt has 5000 nodes, 19983 edges\n",
      "28.txt has 5000 nodes, 19982 edges\n",
      "29.txt has 5000 nodes, 19983 edges\n",
      "3.txt has 5000 nodes, 19982 edges\n",
      "4.txt has 5000 nodes, 19984 edges\n",
      "5.txt has 5000 nodes, 19981 edges\n",
      "6.txt has 5000 nodes, 19984 edges\n",
      "7.txt has 5000 nodes, 19983 edges\n",
      "8.txt has 5000 nodes, 19983 edges\n",
      "9.txt has 5000 nodes, 19984 edges\n",
      "com-youtube.txt has 1134890 nodes, 2987624 edges\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "dpath = \".\\\\data\\\\\"\n",
    "gList = dict()\n",
    "filenames = []\n",
    "\n",
    "for root, dirs, files in os.walk(dpath):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if 'score' not in file:\n",
    "            filenames.append(file)\n",
    "            # Process nodes and edges\n",
    "            gList[file] = dict()\n",
    "            gList[file]['graph']=nx.Graph()\n",
    "            with open(file_path,'r') as f:\n",
    "                content = f.readlines()\n",
    "                edges = []\n",
    "                for line in content:\n",
    "                    if 'com' not in file:\n",
    "                        nodes = line[:-1].split('\\t')\n",
    "                    else:\n",
    "                        #continue # after finish all code run code with com\n",
    "                        nodes = line[:-1].split(\" \")\n",
    "                    # Create edge tuple and append\n",
    "                    edges.append((int(nodes[0]),int(nodes[1])))\n",
    "                gList[file]['graph'].add_edges_from(edges)\n",
    "                print(\"{} has {} nodes, {} edges\".format(file,gList[file]['graph'].number_of_nodes(),gList[file]['graph'].number_of_edges()))\n",
    "            \n",
    "            # Process scores\n",
    "            scorefile = file.replace(\".txt\",\"_score.txt\")\n",
    "            gList[file]['score'] = dict()\n",
    "            score_file_path = os.path.join(root,scorefile) \n",
    "            with open(score_file_path,'r') as f:\n",
    "                content = f.readlines()\n",
    "                for line in content:\n",
    "                    if 'com' not in file:\n",
    "                        node_score = line[:-1].split('\\t')\n",
    "                        gList[file]['score'][int(node_score[0])] = float(node_score[1])\n",
    "                    else:\n",
    "                        #continue # after finish all code run code with com\n",
    "                        node_score = line[:-1].split()\n",
    "                        gList[file]['score'][int(node_score[0][:-1])] = float(node_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cd0956",
   "metadata": {},
   "source": [
    "# 3. DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e286c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = gList['0.txt']['graph']\n",
    "y = torch.tensor(list(gList['0.txt']['score'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72b7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare nodes initial feature X [dv,1,1]\n",
    "def gen_nodes_feature(G):\n",
    "    deg = np.array(list(dict(sorted(dict(G.degree()).items())).values()))\n",
    "    X = np.ones((3,len(deg)))\n",
    "    X[0,:]=deg\n",
    "    norms = np.linalg.norm(X,axis = 1,keepdims=True)\n",
    "    X = torch.FloatTensor(X.T)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f8cc8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d99f2d",
   "metadata": {},
   "source": [
    "## 3a. DrBC encoder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edd220ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the model\\ninput_size = 3\\nhidden_size = 32\\nnum_layers = 5\\nencoder = DrBCEncoder(input_size, hidden_size, num_layers,g)\\nX = gen_nodes_feature(g)\\nout = encoder(X)\\nprint(out.shape)\\nprint(out)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DrBCEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,G):\n",
    "        super(DrBCEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layer1 = nn.Linear(input_size,hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.gru_cell = nn.GRUCell(hidden_size, hidden_size,bias = False)\n",
    "        self.norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.G = G\n",
    "        self.deg = dict(self.G.degree())\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        output = [x]\n",
    "        for i in range(self.num_layers-1):\n",
    "            hn = self.calHn(x)\n",
    "            x = self.gru_cell(x,hn)\n",
    "            x = self.norm2(x)\n",
    "            output.append(x)\n",
    "        output, _ = torch.max(torch.stack(output), dim=0)\n",
    "        return output\n",
    "\n",
    "    def calHn(self,x):\n",
    "        hn = torch.zeros(x.shape).to(self.device)\n",
    "        for node in self.G.nodes():\n",
    "            degv = self.deg[node]\n",
    "            for neigh in list(self.G.adj[node]):\n",
    "                denominator = 1/(math.sqrt(degv+1)*math.sqrt(self.deg[neigh]+1))\n",
    "                hn[node,:] += (denominator*x[neigh])\n",
    "        return hn\n",
    "    \n",
    "'''\n",
    "# Define the model\n",
    "input_size = 3\n",
    "hidden_size = 32\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,g)\n",
    "X = gen_nodes_feature(g)\n",
    "out = encoder(X)\n",
    "print(out.shape)\n",
    "print(out)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81ebdc3",
   "metadata": {},
   "source": [
    "## 3b. Decoder: 2-layer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a345a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrBCDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DrBCDecoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the layers of the decoder\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.norm2 = nn.BatchNorm1d(output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input through the layers of the decoder\n",
    "        x = self.layer1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e17db1",
   "metadata": {},
   "source": [
    "## 3c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8e81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(minimum,maximum,qty):\n",
    "    pairs = []\n",
    "    for i in range(qty):\n",
    "        a = random.randint(minimum,maximum)\n",
    "        b = random.randint(minimum,maximum)\n",
    "        pairs.append((a,b))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "300596cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bc_pairs(pairs, outputs, y):\n",
    "    pred = []\n",
    "    gt = []\n",
    "    g = nn.Sigmoid()\n",
    "    for i, pair in enumerate(pairs):\n",
    "        pred.append(g(outputs[pair[0]] - outputs[pair[1]]))\n",
    "        gt.append(g(y[pair[0]] - y[pair[1]]))\n",
    "    return torch.stack(pred), torch.stack(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec0e095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nG = gList['0.txt']['graph']\\ny = torch.tensor([list(gList['0.txt']['score'].values())])\\ny = torch.transpose(y,0,1)\\n# Define the models\\ninput_size = 3\\nhidden_size = 128\\noutput_size = 1\\nnum_layers = 5\\nencoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\\ndecoder = DrBCDecoder(hidden_size,hidden_size,output_size)\\n\\nn = G.number_of_nodes()\\nnum_episodes = 20\\nlr = 0.001\\nsample_qty = 5*n\\n\\n# Define the loss and optimizer\\ncriterion = nn.BCELoss(reduction = 'sum')\\noptimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\\n\\n# Get the inputs\\ninputs = gen_nodes_feature(G)\\n\\n# Train the model\\nfor episode in range(num_episodes):\\n    # model\\n    outputs = encoder(inputs)\\n    outputs = decoder(outputs)\\n    \\n    pairs = sampling(0,n-1,sample_qty)\\n    pred,gt = bc_pairs(pairs,outputs,y)\\n    loss = criterion(pred,gt)\\n\\n    if ~loss.requires_grad:\\n        loss.requires_grad_()\\n        \\n    # Zero the parameter gradients\\n    optimizer.zero_grad()\\n    loss.backward()\\n    optimizer.step()\\n\\n    # Print statistics\\n    print('[%d] loss: %.4f' %(episode + 1, loss.item()))\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "G = gList['0.txt']['graph']\n",
    "y = torch.tensor([list(gList['0.txt']['score'].values())])\n",
    "y = torch.transpose(y,0,1)\n",
    "# Define the models\n",
    "input_size = 3\n",
    "hidden_size = 128\n",
    "output_size = 1\n",
    "num_layers = 5\n",
    "encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "decoder = DrBCDecoder(hidden_size,hidden_size,output_size)\n",
    "\n",
    "n = G.number_of_nodes()\n",
    "num_episodes = 20\n",
    "lr = 0.001\n",
    "sample_qty = 5*n\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.BCELoss(reduction = 'sum')\n",
    "optimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# Get the inputs\n",
    "inputs = gen_nodes_feature(G)\n",
    "\n",
    "# Train the model\n",
    "for episode in range(num_episodes):\n",
    "    # model\n",
    "    outputs = encoder(inputs)\n",
    "    outputs = decoder(outputs)\n",
    "    \n",
    "    pairs = sampling(0,n-1,sample_qty)\n",
    "    pred,gt = bc_pairs(pairs,outputs,y)\n",
    "    loss = criterion(pred,gt)\n",
    "\n",
    "    if ~loss.requires_grad:\n",
    "        loss.requires_grad_()\n",
    "        \n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print statistics\n",
    "    print('[%d] loss: %.4f' %(episode + 1, loss.item()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e342b506",
   "metadata": {},
   "source": [
    "# 4. Evaluation Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682e9af",
   "metadata": {},
   "source": [
    "## 4a. Top-N% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a618b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(n,pred,gt):\n",
    "    k = math.ceil(pred.size()[0]*n/100)\n",
    "    _,pred_top = torch.topk(pred.view(-1),k=k)\n",
    "    _,gt_top = torch.topk(gt.view(-1),k=k)\n",
    "    intersect = torch.unique(torch.cat((pred_top,gt_top),0))\n",
    "    acc = (2*k-len(intersect))/k\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4c84e",
   "metadata": {},
   "source": [
    "## 4b. Kendall tau distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2f0dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kendall(pred,gt):\n",
    "    pred_ind = torch.argsort(pred)\n",
    "    gt_ind = torch.argsort(gt)\n",
    "    con = 0 # number of concordant pairs\n",
    "    dcor = 0 # number of discordant pairs\n",
    "    n = len(pred_ind)\n",
    "    for i in range(n):\n",
    "        if pred_ind[i] == gt_ind[i]:\n",
    "            con += 1\n",
    "        else:\n",
    "            dcor += 1\n",
    "    ken = 2*(con-dcor)/(n*(n-1))\n",
    "    return ken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e3bda",
   "metadata": {},
   "source": [
    "## 4c. Wall-clock running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c2e6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timer(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        print(f\"Running {func.__name__} ...\", end='\\r')\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} Done in {end - start:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b437c0",
   "metadata": {},
   "source": [
    "# 5. Putting all things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ffab299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33cff5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_result(filename):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    k = [1,5,10]\n",
    "    \n",
    "    print(\"Working on {}\".format(filename))\n",
    "    # Prepare the data\n",
    "    G = gList[filename]['graph']\n",
    "    y = torch.tensor([list(gList[filename]['score'].values())])\n",
    "    y = torch.transpose(y,0,1)\n",
    "\n",
    "    # Define the models\n",
    "    input_size = 3\n",
    "    hidden_size = 128\n",
    "    output_size = 1\n",
    "    num_layers = 5\n",
    "    encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "    decoder = DrBCDecoder(hidden_size,hidden_size,output_size)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    model = EncoderDecoder(encoder,decoder)\n",
    "    model.to(device)\n",
    "    \n",
    "    n = G.number_of_nodes()\n",
    "    num_episodes = 20\n",
    "    lr = 0.001\n",
    "    sample_qty = 5*n\n",
    "\n",
    "    # Define the loss and optimizer\n",
    "    criterion = nn.BCELoss(reduction = 'sum')\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction = 'sum') # Because only pred will go through sigmoid, but ground truth won't\n",
    "    #optimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get the inputs\n",
    "    inputs = gen_nodes_feature(G)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Start training on {}\".format(device))\n",
    "    pairs = sampling(0,n-1,sample_qty)\n",
    "    for episode in range(num_episodes):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # model\n",
    "        outputs = model(inputs)\n",
    "        pred,gt = bc_pairs(pairs,outputs,y)\n",
    "        loss = criterion(pred,gt)\n",
    "        if ~loss.requires_grad:\n",
    "            loss.requires_grad_()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if best_loss>loss:\n",
    "            best_loss = loss\n",
    "            best_out = outputs\n",
    "            #best_model_weights = model.state_dict()\n",
    "            \n",
    "        # Print statistics\n",
    "        #if episode%5 == 4:\n",
    "        print('[%d] loss: %.4f' %(episode + 1, loss.item()))\n",
    "            \n",
    "    #torch.save(best_model_weights, 'best_model.pth')\n",
    "    top1 = topN(1,best_out,y)\n",
    "    top5 = topN(5,best_out,y)\n",
    "    top10 = topN(10,best_out,y)\n",
    "    ken = kendall(best_out,y)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "    return top1,top5,top10,ken,elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ba66c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21299.3242\n",
      "[2] loss: 20293.3203\n",
      "[3] loss: 19834.7461\n",
      "[4] loss: 20113.3145\n",
      "[5] loss: 19897.2500\n",
      "[6] loss: 19670.0391\n",
      "[7] loss: 19622.2090\n",
      "[8] loss: 19558.9668\n",
      "[9] loss: 19447.5742\n",
      "[10] loss: 19196.9609\n",
      "[11] loss: 19075.3496\n",
      "[12] loss: 19002.4883\n",
      "[13] loss: 18924.7070\n",
      "[14] loss: 18872.3555\n",
      "[15] loss: 18871.8789\n",
      "[16] loss: 18807.5664\n",
      "[17] loss: 18825.5742\n",
      "[18] loss: 18782.5781\n",
      "[19] loss: 18779.6562\n",
      "[20] loss: 18753.7617\n",
      "Elapsed time: 688.62 seconds\n",
      "Working on 1.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21423.6211\n",
      "[2] loss: 20592.6738\n",
      "[3] loss: 20577.1133\n",
      "[4] loss: 20718.1211\n",
      "[5] loss: 20638.0352\n",
      "[6] loss: 20539.4785\n",
      "[7] loss: 20480.2754\n",
      "[8] loss: 20426.3945\n",
      "[9] loss: 20355.5352\n",
      "[10] loss: 20243.2383\n",
      "[11] loss: 20065.2578\n",
      "[12] loss: 19572.7617\n",
      "[13] loss: 19522.2227\n",
      "[14] loss: 19483.7812\n",
      "[15] loss: 19448.4453\n",
      "[16] loss: 19326.6797\n",
      "[17] loss: 19170.7754\n",
      "[18] loss: 18969.4336\n",
      "[19] loss: 18927.1445\n",
      "[20] loss: 18865.3711\n",
      "Elapsed time: 681.05 seconds\n",
      "Working on 10.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22213.4570\n",
      "[2] loss: 20632.7461\n",
      "[3] loss: 19887.5391\n",
      "[4] loss: 19744.6992\n",
      "[5] loss: 19487.9180\n",
      "[6] loss: 19427.2383\n",
      "[7] loss: 19411.8457\n",
      "[8] loss: 19313.7852\n",
      "[9] loss: 19059.3750\n",
      "[10] loss: 18954.1875\n",
      "[11] loss: 18917.1953\n",
      "[12] loss: 18887.4277\n",
      "[13] loss: 18858.3203\n",
      "[14] loss: 18818.2656\n",
      "[15] loss: 18776.5508\n",
      "[16] loss: 18742.9102\n",
      "[17] loss: 18720.5469\n",
      "[18] loss: 18658.2188\n",
      "[19] loss: 18054.3828\n",
      "[20] loss: 18002.0684\n",
      "Elapsed time: 679.06 seconds\n",
      "Working on 11.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22459.2344\n",
      "[2] loss: 20915.1074\n",
      "[3] loss: 19161.7715\n",
      "[4] loss: 18874.5430\n",
      "[5] loss: 18858.5898\n",
      "[6] loss: 18802.7734\n",
      "[7] loss: 18773.6758\n",
      "[8] loss: 18741.0137\n",
      "[9] loss: 18701.9102\n",
      "[10] loss: 18653.7852\n",
      "[11] loss: 18478.7734\n",
      "[12] loss: 18450.5918\n",
      "[13] loss: 18419.4727\n",
      "[14] loss: 18391.6738\n",
      "[15] loss: 18364.7480\n",
      "[16] loss: 18340.2051\n",
      "[17] loss: 18194.5000\n",
      "[18] loss: 18180.1035\n",
      "[19] loss: 18160.7402\n",
      "[20] loss: 18142.9648\n",
      "Elapsed time: 690.69 seconds\n",
      "Working on 12.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22343.6055\n",
      "[2] loss: 20290.9062\n",
      "[3] loss: 19964.4023\n",
      "[4] loss: 19828.3184\n",
      "[5] loss: 19693.5625\n",
      "[6] loss: 19713.7715\n",
      "[7] loss: 19626.3203\n",
      "[8] loss: 19536.6133\n",
      "[9] loss: 19490.1758\n",
      "[10] loss: 19409.7617\n",
      "[11] loss: 19250.2227\n",
      "[12] loss: 19177.9141\n",
      "[13] loss: 19112.4551\n",
      "[14] loss: 19060.8359\n",
      "[15] loss: 19014.8496\n",
      "[16] loss: 18739.7500\n",
      "[17] loss: 18680.6602\n",
      "[18] loss: 18493.2734\n",
      "[19] loss: 18116.3203\n",
      "[20] loss: 18034.2383\n",
      "Elapsed time: 678.83 seconds\n",
      "Working on 13.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21791.2852\n",
      "[2] loss: 20119.6992\n",
      "[3] loss: 19508.3535\n",
      "[4] loss: 19638.9238\n",
      "[5] loss: 19285.9102\n",
      "[6] loss: 18821.6562\n",
      "[7] loss: 18692.5508\n",
      "[8] loss: 18575.8867\n",
      "[9] loss: 18476.5625\n",
      "[10] loss: 18398.5977\n",
      "[11] loss: 18339.7910\n",
      "[12] loss: 18286.6133\n",
      "[13] loss: 18259.4648\n",
      "[14] loss: 18242.8633\n",
      "[15] loss: 18234.7500\n",
      "[16] loss: 18228.5527\n",
      "[17] loss: 18209.1289\n",
      "[18] loss: 18194.7676\n",
      "[19] loss: 18186.2168\n",
      "[20] loss: 18176.0742\n",
      "Elapsed time: 680.23 seconds\n",
      "Working on 14.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21122.7578\n",
      "[2] loss: 20770.9805\n",
      "[3] loss: 19878.4453\n",
      "[4] loss: 19703.0234\n",
      "[5] loss: 19650.7344\n",
      "[6] loss: 19517.8047\n",
      "[7] loss: 19291.6094\n",
      "[8] loss: 19163.6816\n",
      "[9] loss: 19102.4023\n",
      "[10] loss: 19046.6250\n",
      "[11] loss: 18999.8789\n",
      "[12] loss: 18957.4785\n",
      "[13] loss: 18896.7773\n",
      "[14] loss: 18850.1367\n",
      "[15] loss: 18806.7969\n",
      "[16] loss: 18556.3184\n",
      "[17] loss: 18544.2832\n",
      "[18] loss: 18496.8398\n",
      "[19] loss: 18462.7676\n",
      "[20] loss: 18431.1211\n",
      "Elapsed time: 685.75 seconds\n",
      "Working on 15.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22500.3184\n",
      "[2] loss: 21325.8555\n",
      "[3] loss: 20584.8926\n",
      "[4] loss: 20435.8379\n",
      "[5] loss: 20101.7344\n",
      "[6] loss: 19912.4238\n",
      "[7] loss: 19762.5781\n",
      "[8] loss: 19648.1309\n",
      "[9] loss: 19561.6094\n",
      "[10] loss: 19487.4766\n",
      "[11] loss: 19407.3516\n",
      "[12] loss: 19332.3711\n",
      "[13] loss: 19263.2988\n",
      "[14] loss: 19195.0312\n",
      "[15] loss: 18947.4688\n",
      "[16] loss: 18609.5918\n",
      "[17] loss: 18506.9219\n",
      "[18] loss: 18425.7930\n",
      "[19] loss: 18390.5977\n",
      "[20] loss: 18362.8203\n",
      "Elapsed time: 684.74 seconds\n",
      "Working on 16.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21546.2090\n",
      "[2] loss: 21277.2793\n",
      "[3] loss: 20751.4180\n",
      "[4] loss: 20561.1777\n",
      "[5] loss: 20527.4023\n",
      "[6] loss: 20475.6074\n",
      "[7] loss: 20424.8867\n",
      "[8] loss: 20388.3086\n",
      "[9] loss: 20350.6094\n",
      "[10] loss: 20315.8594\n",
      "[11] loss: 20287.2617\n",
      "[12] loss: 20257.7109\n",
      "[13] loss: 20232.0273\n",
      "[14] loss: 20207.8105\n",
      "[15] loss: 20185.1816\n",
      "[16] loss: 20192.6680\n",
      "[17] loss: 19597.4102\n",
      "[18] loss: 19560.0352\n",
      "[19] loss: 19136.0918\n",
      "[20] loss: 19094.9688\n",
      "Elapsed time: 688.36 seconds\n",
      "Working on 17.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20669.6055\n",
      "[2] loss: 19836.2266\n",
      "[3] loss: 19529.4414\n",
      "[4] loss: 19369.8320\n",
      "[5] loss: 19296.1211\n",
      "[6] loss: 19233.1426\n",
      "[7] loss: 18989.3672\n",
      "[8] loss: 18961.2734\n",
      "[9] loss: 18935.1211\n",
      "[10] loss: 18903.0469\n",
      "[11] loss: 18869.3242\n",
      "[12] loss: 18837.3047\n",
      "[13] loss: 18812.8945\n",
      "[14] loss: 18419.7812\n",
      "[15] loss: 18391.9336\n",
      "[16] loss: 18362.9570\n",
      "[17] loss: 18336.1484\n",
      "[18] loss: 18299.1641\n",
      "[19] loss: 18255.2266\n",
      "[20] loss: 18222.4062\n",
      "Elapsed time: 687.35 seconds\n",
      "Working on 18.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21000.0977\n",
      "[2] loss: 20434.4336\n",
      "[3] loss: 20178.1406\n",
      "[4] loss: 20241.1680\n",
      "[5] loss: 20101.5195\n",
      "[6] loss: 19966.7461\n",
      "[7] loss: 19926.1758\n",
      "[8] loss: 19697.2344\n",
      "[9] loss: 19572.8047\n",
      "[10] loss: 19483.2070\n",
      "[11] loss: 19388.5117\n",
      "[12] loss: 19318.0586\n",
      "[13] loss: 19297.7227\n",
      "[14] loss: 19250.8027\n",
      "[15] loss: 19199.5312\n",
      "[16] loss: 18293.3008\n",
      "[17] loss: 18522.9844\n",
      "[18] loss: 18664.1016\n",
      "[19] loss: 19170.8086\n",
      "[20] loss: 19167.9375\n",
      "Elapsed time: 691.73 seconds\n",
      "Working on 19.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22102.8477\n",
      "[2] loss: 20576.7383\n",
      "[3] loss: 19873.6816\n",
      "[4] loss: 19729.1543\n",
      "[5] loss: 19560.4629\n",
      "[6] loss: 19457.8750\n",
      "[7] loss: 19413.8672\n",
      "[8] loss: 19350.3145\n",
      "[9] loss: 18908.4102\n",
      "[10] loss: 18871.8027\n",
      "[11] loss: 18809.8047\n",
      "[12] loss: 18630.4492\n",
      "[13] loss: 18492.1914\n",
      "[14] loss: 18471.0352\n",
      "[15] loss: 18544.9023\n",
      "[16] loss: 18505.0430\n",
      "[17] loss: 18469.1094\n",
      "[18] loss: 18302.4395\n",
      "[19] loss: 18175.4609\n",
      "[20] loss: 18175.7676\n",
      "Elapsed time: 689.54 seconds\n",
      "Working on 2.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22154.6875\n",
      "[2] loss: 20700.0195\n",
      "[3] loss: 20589.3105\n",
      "[4] loss: 20278.6250\n",
      "[5] loss: 20135.7500\n",
      "[6] loss: 20054.3945\n",
      "[7] loss: 19967.4023\n",
      "[8] loss: 19803.7520\n",
      "[9] loss: 19643.7559\n",
      "[10] loss: 19588.2031\n",
      "[11] loss: 19541.9512\n",
      "[12] loss: 19501.2539\n",
      "[13] loss: 19458.2344\n",
      "[14] loss: 19434.1152\n",
      "[15] loss: 19386.1953\n",
      "[16] loss: 19368.3281\n",
      "[17] loss: 18900.6152\n",
      "[18] loss: 18869.2188\n",
      "[19] loss: 18838.3672\n",
      "[20] loss: 18636.5195\n",
      "Elapsed time: 682.26 seconds\n",
      "Working on 20.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21842.3750\n",
      "[2] loss: 20631.3516\n",
      "[3] loss: 20156.5957\n",
      "[4] loss: 19920.4805\n",
      "[5] loss: 19802.3730\n",
      "[6] loss: 19728.7012\n",
      "[7] loss: 19670.5059\n",
      "[8] loss: 19618.5723\n",
      "[9] loss: 19572.5410\n",
      "[10] loss: 19497.7305\n",
      "[11] loss: 19391.4961\n",
      "[12] loss: 19365.2773\n",
      "[13] loss: 19354.3281\n",
      "[14] loss: 19318.0703\n",
      "[15] loss: 19284.0898\n",
      "[16] loss: 19081.8672\n",
      "[17] loss: 19059.6113\n",
      "[18] loss: 18819.6348\n",
      "[19] loss: 18811.9707\n",
      "[20] loss: 18518.7461\n",
      "Elapsed time: 692.70 seconds\n",
      "Working on 21.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21419.7227\n",
      "[2] loss: 20466.8262\n",
      "[3] loss: 19690.7676\n",
      "[4] loss: 19523.3125\n",
      "[5] loss: 19442.9609\n",
      "[6] loss: 19384.0977\n",
      "[7] loss: 19300.0312\n",
      "[8] loss: 19236.3828\n",
      "[9] loss: 19073.1055\n",
      "[10] loss: 18996.2441\n",
      "[11] loss: 18962.0742\n",
      "[12] loss: 18910.4707\n",
      "[13] loss: 18854.9805\n",
      "[14] loss: 18817.6016\n",
      "[15] loss: 18794.1953\n",
      "[16] loss: 18758.6602\n",
      "[17] loss: 18548.1113\n",
      "[18] loss: 18492.4395\n",
      "[19] loss: 18455.1660\n",
      "[20] loss: 18425.1992\n",
      "Elapsed time: 681.30 seconds\n",
      "Working on 22.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21633.8203\n",
      "[2] loss: 20389.9082\n",
      "[3] loss: 19766.7012\n",
      "[4] loss: 19585.2012\n",
      "[5] loss: 19434.3184\n",
      "[6] loss: 19315.2617\n",
      "[7] loss: 19220.0000\n",
      "[8] loss: 19156.2988\n",
      "[9] loss: 19075.8438\n",
      "[10] loss: 19008.7188\n",
      "[11] loss: 18955.7812\n",
      "[12] loss: 18910.2656\n",
      "[13] loss: 18846.8457\n",
      "[14] loss: 18809.9355\n",
      "[15] loss: 18777.6934\n",
      "[16] loss: 18770.2402\n",
      "[17] loss: 18744.2500\n",
      "[18] loss: 18729.0898\n",
      "[19] loss: 18723.3281\n",
      "[20] loss: 18707.3398\n",
      "Elapsed time: 681.79 seconds\n",
      "Working on 23.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21282.6133\n",
      "[2] loss: 20543.0332\n",
      "[3] loss: 20241.5352\n",
      "[4] loss: 20049.4375\n",
      "[5] loss: 19938.5117\n",
      "[6] loss: 19810.9844\n",
      "[7] loss: 19699.3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8] loss: 19609.6055\n",
      "[9] loss: 19525.5879\n",
      "[10] loss: 19224.6172\n",
      "[11] loss: 19155.1758\n",
      "[12] loss: 19098.3730\n",
      "[13] loss: 19051.4570\n",
      "[14] loss: 18996.3438\n",
      "[15] loss: 18942.3242\n",
      "[16] loss: 18913.7852\n",
      "[17] loss: 18905.1328\n",
      "[18] loss: 18848.3184\n",
      "[19] loss: 18674.0000\n",
      "[20] loss: 18855.9531\n",
      "Elapsed time: 678.45 seconds\n",
      "Working on 24.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22355.7891\n",
      "[2] loss: 20697.1953\n",
      "[3] loss: 20376.0547\n",
      "[4] loss: 20254.7402\n",
      "[5] loss: 20122.3398\n",
      "[6] loss: 20013.2891\n",
      "[7] loss: 19859.1797\n",
      "[8] loss: 19737.7188\n",
      "[9] loss: 19626.5586\n",
      "[10] loss: 19518.3242\n",
      "[11] loss: 19439.8691\n",
      "[12] loss: 19196.6211\n",
      "[13] loss: 19142.7285\n",
      "[14] loss: 18794.9219\n",
      "[15] loss: 18753.7695\n",
      "[16] loss: 18490.3477\n",
      "[17] loss: 18716.0508\n",
      "[18] loss: 18713.5781\n",
      "[19] loss: 18662.0117\n",
      "[20] loss: 18415.9648\n",
      "Elapsed time: 679.51 seconds\n",
      "Working on 25.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22402.2148\n",
      "[2] loss: 21026.3711\n",
      "[3] loss: 20166.7812\n",
      "[4] loss: 19919.0273\n",
      "[5] loss: 19628.1367\n",
      "[6] loss: 19479.2676\n",
      "[7] loss: 19385.6680\n",
      "[8] loss: 19285.4844\n",
      "[9] loss: 19115.6934\n",
      "[10] loss: 19026.5742\n",
      "[11] loss: 18758.9883\n",
      "[12] loss: 18694.3555\n",
      "[13] loss: 18612.0469\n",
      "[14] loss: 18536.0273\n",
      "[15] loss: 18453.5742\n",
      "[16] loss: 18394.6562\n",
      "[17] loss: 18336.7793\n",
      "[18] loss: 18306.4648\n",
      "[19] loss: 18274.9766\n",
      "[20] loss: 18269.3086\n",
      "Elapsed time: 649.22 seconds\n",
      "Working on 26.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21943.0078\n",
      "[2] loss: 20726.7188\n",
      "[3] loss: 20391.9785\n",
      "[4] loss: 20379.5859\n",
      "[5] loss: 20330.4395\n",
      "[6] loss: 20231.1328\n",
      "[7] loss: 20153.8145\n",
      "[8] loss: 20107.8516\n",
      "[9] loss: 20097.8750\n",
      "[10] loss: 20060.6816\n",
      "[11] loss: 20057.6973\n",
      "[12] loss: 20011.7695\n",
      "[13] loss: 19887.2773\n",
      "[14] loss: 19676.2949\n",
      "[15] loss: 19622.7051\n",
      "[16] loss: 19573.9453\n",
      "[17] loss: 19477.5312\n",
      "[18] loss: 19429.3379\n",
      "[19] loss: 19377.2227\n",
      "[20] loss: 19345.7891\n",
      "Elapsed time: 653.88 seconds\n",
      "Working on 27.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21985.3027\n",
      "[2] loss: 20854.7109\n",
      "[3] loss: 20285.4316\n",
      "[4] loss: 20050.5742\n",
      "[5] loss: 19987.0000\n",
      "[6] loss: 19944.0234\n",
      "[7] loss: 19884.1992\n",
      "[8] loss: 19829.3125\n",
      "[9] loss: 19767.7734\n",
      "[10] loss: 19709.9043\n",
      "[11] loss: 19668.7422\n",
      "[12] loss: 19624.0430\n",
      "[13] loss: 19568.5898\n",
      "[14] loss: 19505.0039\n",
      "[15] loss: 19438.0762\n",
      "[16] loss: 19388.1758\n",
      "[17] loss: 19328.3145\n",
      "[18] loss: 19106.1406\n",
      "[19] loss: 19061.4961\n",
      "[20] loss: 18860.9570\n",
      "Elapsed time: 645.70 seconds\n",
      "Working on 28.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20591.7305\n",
      "[2] loss: 20218.5137\n",
      "[3] loss: 19880.5938\n",
      "[4] loss: 19652.8145\n",
      "[5] loss: 19637.5840\n",
      "[6] loss: 19569.6270\n",
      "[7] loss: 19501.1680\n",
      "[8] loss: 19462.4492\n",
      "[9] loss: 19440.2031\n",
      "[10] loss: 19409.5039\n",
      "[11] loss: 19368.2305\n",
      "[12] loss: 19333.3672\n",
      "[13] loss: 19326.2266\n",
      "[14] loss: 19302.6387\n",
      "[15] loss: 19262.9766\n",
      "[16] loss: 19227.5391\n",
      "[17] loss: 19191.3633\n",
      "[18] loss: 19158.0859\n",
      "[19] loss: 18515.8262\n",
      "[20] loss: 18470.1797\n",
      "Elapsed time: 644.13 seconds\n",
      "Working on 29.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21724.5898\n",
      "[2] loss: 20960.1348\n",
      "[3] loss: 20805.2793\n",
      "[4] loss: 20607.3438\n",
      "[5] loss: 20498.0781\n",
      "[6] loss: 20406.3477\n",
      "[7] loss: 20325.2285\n",
      "[8] loss: 20303.7246\n",
      "[9] loss: 20278.8555\n",
      "[10] loss: 20264.3711\n",
      "[11] loss: 20247.4375\n",
      "[12] loss: 20151.9727\n",
      "[13] loss: 20136.9590\n",
      "[14] loss: 20002.3965\n",
      "[15] loss: 19879.5820\n",
      "[16] loss: 19838.5996\n",
      "[17] loss: 19790.4551\n",
      "[18] loss: 19738.1289\n",
      "[19] loss: 19688.4336\n",
      "[20] loss: 19654.2070\n",
      "Elapsed time: 646.97 seconds\n",
      "Working on 3.txt\n",
      "Start training on cuda\n",
      "[1] loss: 20862.7773\n",
      "[2] loss: 19942.7676\n",
      "[3] loss: 19698.1348\n",
      "[4] loss: 19511.5293\n",
      "[5] loss: 19391.2598\n",
      "[6] loss: 19263.3867\n",
      "[7] loss: 19119.9395\n",
      "[8] loss: 19002.1133\n",
      "[9] loss: 18582.1172\n",
      "[10] loss: 18487.2109\n",
      "[11] loss: 18449.9395\n",
      "[12] loss: 18375.6367\n",
      "[13] loss: 18344.2871\n",
      "[14] loss: 18323.0938\n",
      "[15] loss: 18308.3047\n",
      "[16] loss: 18289.4199\n",
      "[17] loss: 18285.7461\n",
      "[18] loss: 18279.1230\n",
      "[19] loss: 18269.7344\n",
      "[20] loss: 18267.4316\n",
      "Elapsed time: 648.63 seconds\n",
      "Working on 4.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21585.6504\n",
      "[2] loss: 19773.6191\n",
      "[3] loss: 19371.0547\n",
      "[4] loss: 19207.7500\n",
      "[5] loss: 19028.4414\n",
      "[6] loss: 18895.5176\n",
      "[7] loss: 18790.1016\n",
      "[8] loss: 18706.9590\n",
      "[9] loss: 18329.1035\n",
      "[10] loss: 18262.4023\n",
      "[11] loss: 18227.4121\n",
      "[12] loss: 18181.1641\n",
      "[13] loss: 18123.5703\n",
      "[14] loss: 18089.7715\n",
      "[15] loss: 18076.7012\n",
      "[16] loss: 18053.6875\n",
      "[17] loss: 18043.2383\n",
      "[18] loss: 18039.3125\n",
      "[19] loss: 17842.6680\n",
      "[20] loss: 17686.1660\n",
      "Elapsed time: 652.09 seconds\n",
      "Working on 5.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21378.4785\n",
      "[2] loss: 20301.5410\n",
      "[3] loss: 19845.9609\n",
      "[4] loss: 19646.5195\n",
      "[5] loss: 19533.6113\n",
      "[6] loss: 19432.7871\n",
      "[7] loss: 19352.6758\n",
      "[8] loss: 19282.5508\n",
      "[9] loss: 19194.9082\n",
      "[10] loss: 19133.8340\n",
      "[11] loss: 19095.3359\n",
      "[12] loss: 19051.4062\n",
      "[13] loss: 19000.1777\n",
      "[14] loss: 18952.0859\n",
      "[15] loss: 18668.5254\n",
      "[16] loss: 18655.4180\n",
      "[17] loss: 18607.3320\n",
      "[18] loss: 18585.3867\n",
      "[19] loss: 18307.6758\n",
      "[20] loss: 18311.5977\n",
      "Elapsed time: 646.15 seconds\n",
      "Working on 6.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21479.5781\n",
      "[2] loss: 20711.6523\n",
      "[3] loss: 20643.0703\n",
      "[4] loss: 20504.2070\n",
      "[5] loss: 20365.3809\n",
      "[6] loss: 20225.1211\n",
      "[7] loss: 19751.1523\n",
      "[8] loss: 19697.9648\n",
      "[9] loss: 19604.0957\n",
      "[10] loss: 19541.0312\n",
      "[11] loss: 19213.2500\n",
      "[12] loss: 19066.3965\n",
      "[13] loss: 19013.9766\n",
      "[14] loss: 18708.3809\n",
      "[15] loss: 18722.5273\n",
      "[16] loss: 18828.3086\n",
      "[17] loss: 18703.8906\n",
      "[18] loss: 18588.7305\n",
      "[19] loss: 18299.4844\n",
      "[20] loss: 18269.9902\n",
      "Elapsed time: 657.59 seconds\n",
      "Working on 7.txt\n",
      "Start training on cuda\n",
      "[1] loss: 21178.1523\n",
      "[2] loss: 20467.1777\n",
      "[3] loss: 20231.0977\n",
      "[4] loss: 20127.6172\n",
      "[5] loss: 20038.7344\n",
      "[6] loss: 19940.1836\n",
      "[7] loss: 19856.9219\n",
      "[8] loss: 19705.2539\n",
      "[9] loss: 19452.2109\n",
      "[10] loss: 19416.8164\n",
      "[11] loss: 19380.2500\n",
      "[12] loss: 19342.8926\n",
      "[13] loss: 19320.6406\n",
      "[14] loss: 19294.3398\n",
      "[15] loss: 19277.0176\n",
      "[16] loss: 19255.8594\n",
      "[17] loss: 19235.5430\n",
      "[18] loss: 19216.0508\n",
      "[19] loss: 19193.2773\n",
      "[20] loss: 19102.3242\n",
      "Elapsed time: 656.07 seconds\n",
      "Working on 8.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22349.1445\n",
      "[2] loss: 21527.0957\n",
      "[3] loss: 20833.3047\n",
      "[4] loss: 20342.7559\n",
      "[5] loss: 20107.3906\n",
      "[6] loss: 20154.6348\n",
      "[7] loss: 20204.3867\n",
      "[8] loss: 20149.0039\n",
      "[9] loss: 19855.8711\n",
      "[10] loss: 19663.1797\n",
      "[11] loss: 19420.4727\n",
      "[12] loss: 19341.7695\n",
      "[13] loss: 19261.3652\n",
      "[14] loss: 19170.6348\n",
      "[15] loss: 19092.1641\n",
      "[16] loss: 18820.7285\n",
      "[17] loss: 18738.5820\n",
      "[18] loss: 18685.1172\n",
      "[19] loss: 18606.1328\n",
      "[20] loss: 18566.3535\n",
      "Elapsed time: 658.16 seconds\n",
      "Working on 9.txt\n",
      "Start training on cuda\n",
      "[1] loss: 22009.8027\n",
      "[2] loss: 20267.5801\n",
      "[3] loss: 19974.7422\n",
      "[4] loss: 19839.5977\n",
      "[5] loss: 19736.3477\n",
      "[6] loss: 19651.5078\n",
      "[7] loss: 19565.7305\n",
      "[8] loss: 19526.1641\n",
      "[9] loss: 19466.2949\n",
      "[10] loss: 19240.6855\n",
      "[11] loss: 19183.1328\n",
      "[12] loss: 19104.6094\n",
      "[13] loss: 18949.5469\n",
      "[14] loss: 18930.5879\n",
      "[15] loss: 18768.7188\n",
      "[16] loss: 18695.0898\n",
      "[17] loss: 18640.1211\n",
      "[18] loss: 18285.8848\n",
      "[19] loss: 17978.9297\n",
      "[20] loss: 17972.2305\n",
      "Elapsed time: 652.54 seconds\n"
     ]
    }
   ],
   "source": [
    "top1_list = []\n",
    "top5_list = []\n",
    "top10_list = []\n",
    "ken_list = []\n",
    "elapsed_time_list = []\n",
    "for filename in filenames[:-1]:\n",
    "    top1,top5,top10,ken,elapsed_time = train_and_result(filename)\n",
    "    top1_list.append(top1)\n",
    "    top5_list.append(top5)\n",
    "    top10_list.append(top10)\n",
    "    ken_list.append(ken)\n",
    "    elapsed_time_list.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e9a0bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1% accuracy: 0.28±0.18\n",
      "Top-5% accuracy: 0.19±0.09\n",
      "Top-10% accuracy: 0.22±0.07\n",
      "Kendall tau distance: 0.00±0.00\n",
      "Running time: 671.10±17.12\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mean for each evaluation metrics:\n",
    "print(\"Top-1% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top1_list)),np.std(np.array(top1_list))))\n",
    "print(\"Top-5% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top5_list)),np.std(np.array(top5_list))))\n",
    "print(\"Top-10% accuracy: {:.2f}±{:.2f}\".format(np.mean(np.array(top10_list)),np.std(np.array(top10_list))))\n",
    "print(\"Kendall tau distance: {:.2f}±{:.2f}\".format(np.mean(np.array(ken_list)),np.std(np.array(ken_list))))\n",
    "print(\"Running time: {:.2f}±{:.2f}\".format(np.mean(np.array(elapsed_time_list)),np.std(np.array(elapsed_time_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047c1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_result_for_com(filename):\n",
    "    start_time = time.time()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    k = [1,5,10]\n",
    "    \n",
    "    print(\"Working on {}\".format(filename))\n",
    "    # Prepare the data\n",
    "    G = gList[filename]['graph']\n",
    "    y = torch.tensor([list(gList[filename]['score'].values())])\n",
    "    y = torch.transpose(y,0,1)\n",
    "\n",
    "    # Define the models\n",
    "    input_size = 3\n",
    "    hidden_size = 3\n",
    "    output_size = 1\n",
    "    num_layers = 1\n",
    "    encoder = DrBCEncoder(input_size, hidden_size, num_layers,G)\n",
    "    decoder = DrBCDecoder(hidden_size,hidden_size,output_size)\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    model = EncoderDecoder(encoder,decoder)\n",
    "    model.to(device)\n",
    "    \n",
    "    n = G.number_of_nodes()\n",
    "    num_episodes = 5\n",
    "    lr = 0.001\n",
    "    sample_qty = int(0.5*n) # Reduce to 1.5|N|\n",
    "\n",
    "    # Define the loss and optimizer\n",
    "    criterion = nn.BCELoss(reduction = 'mean') # Sum may leak\n",
    "    #criterion = nn.BCEWithLogitsLoss(reduction = 'sum') # Because only pred will go through sigmoid, but ground truth won't\n",
    "    #optimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()), lr=lr)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Get the inputs\n",
    "    inputs = gen_nodes_feature(G)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Start training on {}\".format(device))\n",
    "    pairs = sampling(0,n-1,sample_qty)\n",
    "    for episode in range(num_episodes):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # model\n",
    "        outputs = model(inputs)\n",
    "        print(\"finish output from model\")\n",
    "        pred,gt = bc_pairs(pairs,outputs,y)\n",
    "        loss = criterion(pred,gt)\n",
    "        if ~loss.requires_grad:\n",
    "            loss.requires_grad_()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if best_loss>loss:\n",
    "            best_loss = loss\n",
    "            best_out = outputs\n",
    "            #best_model_weights = model.state_dict()\n",
    "            \n",
    "        # Print statistics\n",
    "        #if episode%5 == 4:\n",
    "        print('[%d] loss: %.4f' %(episode + 1, loss.item()))\n",
    "            \n",
    "    #torch.save(best_model_weights, 'best_model.pth')\n",
    "    top1 = topN(1,best_out,y)\n",
    "    top5 = topN(5,best_out,y)\n",
    "    top10 = topN(10,best_out,y)\n",
    "    ken = kendall(best_out,y)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time-start_time\n",
    "    print(\"Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "    return top1,top5,top10,ken,elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c277300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on com-youtube.txt\n",
      "Start training on cuda\n"
     ]
    }
   ],
   "source": [
    "com_top1,com_top5,com_top10,com_ken,com_elapsed_time = train_and_result(filenames[-1])\n",
    "print(\"Top-1% accuracy: {:.2f}\".format(com_top1))\n",
    "print(\"Top-5% accuracy: {:.2f}\".format(com_top5))\n",
    "print(\"Top-10% accuracy: {:.2f}\".format(com_top10))\n",
    "print(\"Kendall tau distance: {:.2f}\".format(com_ken))\n",
    "print(\"Running time: {:.2f}\".format(com_elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e76ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
